{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cfe317",
   "metadata": {},
   "source": [
    "![Status: Complete](https://img.shields.io/badge/status-complete-brightgreen)\n",
    "![Python](https://img.shields.io/badge/python-3.10-blue)\n",
    "![Coverage](https://img.shields.io/badge/coverage-80%25-yellowgreen)\n",
    "![License](https://img.shields.io/badge/license-MIT-green)\n",
    "\n",
    "<a id=\"table-of-contents\"></a>\n",
    "# ğŸ“– Outlier Detection & Treatment\n",
    "\n",
    "- [ğŸ” Statistical Detection Methods](#statistical-methods)\n",
    "  - [ğŸ“ˆ Z-Score](#z-score)\n",
    "  - [ğŸ§® Modified Z-Score (MAD)](#modified-z-score)\n",
    "  - [ğŸ“Š Interquartile Range (IQR)](#iqr)\n",
    "  - [ğŸ” Grubbs' Test](#grubbs-test)\n",
    "  - [ğŸ“ Chi-Square & Mahalanobis Distance](#chi-square-mahalanobis)\n",
    "- [ğŸ¤– ML-Based Detection Methods](#ml-methods)\n",
    "  - [ğŸŒ² Isolation Forest](#isolation-forest)\n",
    "  - [ğŸ” Local Outlier Factor (LOF)](#lof)\n",
    "- [ğŸ“ Proximity & Clustering-Based Detection](#proximity-methods)\n",
    "  - [ğŸŒ€ DBSCAN](#dbscan)\n",
    "  - [ğŸ“ K-Means Distance](#kmeans-distance)\n",
    "- [ğŸ“ Probabilistic Detection Methods](#probabilistic-methods)\n",
    "  - [ğŸ“Š Gaussian Mixture Models (GMM)](#gmm)\n",
    "  - [ğŸ“‰ Extreme Value Theory (EVT)](#evt)\n",
    "  - [ğŸ§  Bayesian Methods](#bayesian)\n",
    "- [ğŸ› ï¸ Outlier Treatment Strategies](#treatment-strategies)\n",
    "  - [âŒ Deletion](#deletion)\n",
    "  - [ğŸ” Capping / Winsorizing](#capping)\n",
    "  - [ğŸ§® Imputation](#imputation)\n",
    "  - [ğŸ“Š Binning](#binning)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250e198c",
   "metadata": {},
   "source": [
    "<a id=\"statistical-methods\"></a>\n",
    "# ğŸ” Statistical Detection Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668458ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.967142</td>\n",
       "      <td>22.923146</td>\n",
       "      <td>107.155747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.617357</td>\n",
       "      <td>27.896773</td>\n",
       "      <td>111.215691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.476885</td>\n",
       "      <td>28.286427</td>\n",
       "      <td>121.661025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65.230299</td>\n",
       "      <td>25.988614</td>\n",
       "      <td>121.076041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.658466</td>\n",
       "      <td>29.193571</td>\n",
       "      <td>72.446613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2   feature_3\n",
       "0  54.967142  22.923146  107.155747\n",
       "1  48.617357  27.896773  111.215691\n",
       "2  56.476885  28.286427  121.661025\n",
       "3  65.230299  25.988614  121.076041\n",
       "4  47.658466  29.193571   72.446613"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dummy dataset with multivariate numeric features\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate base normal data\n",
    "data = {\n",
    "    'feature_1': np.random.normal(50, 10, 100),\n",
    "    'feature_2': np.random.normal(30, 5, 100),\n",
    "    'feature_3': np.random.normal(100, 20, 100)\n",
    "}\n",
    "\n",
    "# Inject outliers\n",
    "data['feature_1'][[5, 15]] = [150, -30]\n",
    "data['feature_2'][[25]] = [100]\n",
    "data['feature_3'][[70, 90]] = [250, -80]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e47fdc6",
   "metadata": {},
   "source": [
    "<a id=\"z-score\"></a>\n",
    "#### ğŸ“ˆ Z-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac9b5fb",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“ˆ What is Z-Score?\n",
    "\n",
    "Z-Score measures how many standard deviations a data point is from the mean of the distribution.  \n",
    "It assumes the data follows a normal (Gaussian) distribution.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Calculate mean (Î¼) and standard deviation (Ïƒ) of the feature\n",
    "- For each value \\( x \\), compute \\( z = \\frac{x - \\mu}{\\sigma} \\)\n",
    "- Points with |z| > threshold (typically 3) are flagged as outliers\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When the variable is **normally distributed**\n",
    "- Works well for **univariate** outlier detection\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Simple and interpretable\n",
    "- Requires no parameter tuning beyond threshold\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Sensitive to mean and standard deviation â€” affected by extreme values\n",
    "- Fails on skewed or non-Gaussian data\n",
    "- Can miss multivariate outliers\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46017a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [feature_1] - Detected 2 outliers using Z-Score (threshold = 3.0)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_2] - Detected 1 outliers using Z-Score (threshold = 3.0)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_3] - Detected 2 outliers using Z-Score (threshold = 3.0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "def detect_outliers_zscore_all(df, column, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers in a single numeric column of a DataFrame using Z-Score.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Name of the numeric column.\n",
    "        threshold (float): Z-score threshold (default is 3.0).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier in the specified column.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Column '{column}' is either missing or not numeric.\")\n",
    "\n",
    "    z_scores = zscore(df[column])\n",
    "    mask = np.abs(z_scores) > threshold\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Detected {mask.sum()} outliers using Z-Score (threshold = {threshold})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - No outliers detected using Z-Score (threshold = {threshold})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Run Z-Score detection across all numeric columns\n",
    "zscore_results = {}\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    zscore_results[col] = detect_outliers_zscore_all(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dad288",
   "metadata": {},
   "source": [
    "<a id=\"modified-z-score\"></a>\n",
    "#### ğŸ§® Modified Z-Score (MAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f0ad0",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ§® What is Modified Z-Score?\n",
    "\n",
    "Modified Z-Score replaces the mean and standard deviation in the Z-Score formula with the **median** and **median absolute deviation (MAD)**, making it robust to extreme values.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Compute the median (M) of the data\n",
    "- Calculate MAD = median(|xáµ¢ - M|)\n",
    "- For each point \\( x \\), compute:  \n",
    "  \\( \\text{Modified Z} = 0.6745 \\cdot \\frac{x - M}{\\text{MAD}} \\)\n",
    "- Flag points where |Modified Z| > threshold (typically 3.5)\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When the distribution is **skewed or contains outliers**\n",
    "- Works well for **robust univariate detection**\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Resistant to influence from extreme values\n",
    "- More reliable on small or noisy datasets\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Assumes unimodal structure; may struggle with multimodal data\n",
    "- Still a univariate method â€” doesn't capture contextual or multivariate outliers\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ca1624c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [feature_1] - Detected 2 outliers using Modified Z-Score (threshold = 3.5)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_2] - Detected 1 outliers using Modified Z-Score (threshold = 3.5)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_3] - Detected 2 outliers using Modified Z-Score (threshold = 3.5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_modified_zscore(df, column, threshold=3.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using the Modified Z-Score method based on MAD.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Name of the numeric column.\n",
    "        threshold (float): Modified Z-Score threshold (default is 3.5).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Column '{column}' is either missing or not numeric.\")\n",
    "\n",
    "    series = df[column]\n",
    "    median = series.median()\n",
    "    mad = median_abs_deviation(series, scale='normal')  # default scale approximates std\n",
    "    if mad == 0:\n",
    "        mask = pd.Series([False] * len(series), index=series.index)\n",
    "        print(f\"\\033[93mâš ï¸ [{column}] - MAD is zero. Skipping detection.\\033[0m\")\n",
    "        return mask\n",
    "\n",
    "    modified_z = 0.6745 * (series - median) / mad\n",
    "    mask = np.abs(modified_z) > threshold\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Detected {mask.sum()} outliers using Modified Z-Score (threshold = {threshold})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - No outliers detected using Modified Z-Score (threshold = {threshold})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Run Modified Z-Score detection across all numeric columns\n",
    "mad_results = {}\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    mad_results[col] = detect_outliers_modified_zscore(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68febdc1",
   "metadata": {},
   "source": [
    "<a id=\"iqr\"></a>\n",
    "#### ğŸ“Š Interquartile Range (IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d755973",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“Š What is IQR?\n",
    "\n",
    "The Interquartile Range (IQR) is a rule-based method that defines outliers as points lying far outside the central 50% of the data.  \n",
    "It does not assume any specific distribution.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Compute Q1 (25th percentile) and Q3 (75th percentile)\n",
    "- Calculate IQR = Q3 - Q1\n",
    "- Define lower bound = Q1 - 1.5 Ã— IQR  \n",
    "  Define upper bound = Q3 + 1.5 Ã— IQR\n",
    "- Flag points outside this range as outliers\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- For **non-parametric**, univariate outlier detection\n",
    "- Effective when data is **not normally distributed**\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Simple and interpretable\n",
    "- Not influenced by extreme values\n",
    "- No distributional assumptions\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Doesn't work well with **multimodal** distributions\n",
    "- Only considers 1 feature at a time (univariate)\n",
    "- Threshold (1.5Ã—IQR) is arbitrary and may require tuning\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "984a6a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [feature_1] - Detected 3 outliers using IQR (factor = 1.5)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_2] - Detected 2 outliers using IQR (factor = 1.5)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_3] - Detected 4 outliers using IQR (factor = 1.5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_iqr(df, column, factor=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Name of the numeric column.\n",
    "        factor (float): Multiplier for IQR (default is 1.5).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Column '{column}' is either missing or not numeric.\")\n",
    "\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - factor * IQR\n",
    "    upper = Q3 + factor * IQR\n",
    "\n",
    "    mask = (df[column] < lower) | (df[column] > upper)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Detected {mask.sum()} outliers using IQR (factor = {factor})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - No outliers detected using IQR (factor = {factor})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Run IQR detection across all numeric columns\n",
    "iqr_results = {}\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    iqr_results[col] = detect_outliers_iqr(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b84ed2",
   "metadata": {},
   "source": [
    "<a id=\"grubbs-test\"></a>\n",
    "#### ğŸ” Grubbs' Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95d7683",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ” What is Grubbs' Test?\n",
    "\n",
    "Grubbs' Test is a statistical test used to detect a **single outlier** in a normally distributed univariate dataset.  \n",
    "It evaluates whether the extreme value is statistically different from the rest of the data.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Assumes data follows a **normal distribution**\n",
    "- Calculates a test statistic based on the most extreme value:\n",
    "  \\( G = \\frac{|\\text{extreme} - \\bar{x}|}{s} \\)\n",
    "- Compares \\( G \\) to a critical value from the t-distribution\n",
    "- If \\( G \\) exceeds the threshold, the point is flagged as an outlier\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- On **small, normally distributed** datasets\n",
    "- When verifying if a specific extreme point is statistically unjustified\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Provides a statistical test (p-value) rather than just a threshold rule\n",
    "- Good for auditing one suspect value at a time\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Can only detect **one outlier at a time**\n",
    "- Assumes normality â€” breaks down if data is skewed or multimodal\n",
    "- Not scalable to large or multivariate datasets\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9e1c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [feature_1] - Grubbs' Test: 1 outlier detected (G = 6.39, critical = 3.38)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_2] - Grubbs' Test: 1 outlier detected (G = 8.25, critical = 3.38)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_3] - Grubbs' Test: 1 outlier detected (G = 5.67, critical = 3.38)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t, norm\n",
    "\n",
    "def detect_outliers_grubbs(df, column, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Detect a single outlier in a numeric column using Grubbs' Test.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Name of the numeric column.\n",
    "        alpha (float): Significance level (default is 0.05).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Column '{column}' is either missing or not numeric.\")\n",
    "\n",
    "    x = df[column].dropna().copy()\n",
    "    n = len(x)\n",
    "    mean_x = x.mean()\n",
    "    std_x = x.std()\n",
    "    \n",
    "    # Grubbs statistic\n",
    "    abs_devs = abs(x - mean_x)\n",
    "    G = abs_devs.max() / std_x\n",
    "\n",
    "    # Critical value\n",
    "    t_crit = t.ppf(1 - alpha / (2 * n), df=n - 2)\n",
    "    critical_value = ((n - 1) / np.sqrt(n)) * np.sqrt(t_crit**2 / (n - 2 + t_crit**2))\n",
    "\n",
    "    mask = abs_devs == abs_devs.max()\n",
    "    if G > critical_value:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Grubbs' Test: 1 outlier detected (G = {G:.2f}, critical = {critical_value:.2f})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - Grubbs' Test: No outlier detected (G = {G:.2f}, critical = {critical_value:.2f})\\033[0m\")\n",
    "        mask[:] = False\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Run Grubbs' Test across all numeric columns\n",
    "grubbs_results = {}\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    grubbs_results[col] = detect_outliers_grubbs(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a7683d",
   "metadata": {},
   "source": [
    "<a id=\"chi-square-mahalanobis\"></a>\n",
    "#### ğŸ“ Chi-Square & Mahalanobis Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3dffeb",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“ What Are Chi-Square & Mahalanobis Distance?\n",
    "\n",
    "Both are **multivariate** statistical techniques for detecting outliers by measuring how far a point is from the expected distribution in a multidimensional space.\n",
    "\n",
    "- **Mahalanobis Distance** accounts for correlation between features\n",
    "- **Chi-Square Test** uses Mahalanobis distance under the assumption of multivariate normality\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Compute the mean vector and covariance matrix of the dataset\n",
    "- For each point \\( x \\), compute Mahalanobis distance:\n",
    "  \\( D^2 = (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\)\n",
    "- Compare \\( D^2 \\) to a **Chi-Square threshold** with `df = number of features`\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- Detecting outliers in **multivariate numeric data**\n",
    "- When feature correlation is important (e.g., financial indicators)\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Captures **multivariate outliers** missed by univariate methods\n",
    "- Incorporates feature relationships via covariance\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Assumes multivariate **normal distribution**\n",
    "- Sensitive to outliers in the covariance matrix\n",
    "- Requires numerical, continuous variables\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d7d3e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [Mahalanobis] - Detected 5 multivariate outliers (alpha = 0.01)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "def detect_outliers_mahalanobis(df, columns, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Detect multivariate outliers using Mahalanobis distance and Chi-Square threshold.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of numeric columns to include.\n",
    "        alpha (float): Significance level for Chi-Square test (default 0.01).\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    x = data.values\n",
    "    mean_vec = np.mean(x, axis=0)\n",
    "    cov_matrix = np.cov(x, rowvar=False)\n",
    "    inv_covmat = np.linalg.inv(cov_matrix)\n",
    "\n",
    "    diff = x - mean_vec\n",
    "    left = np.dot(diff, inv_covmat)\n",
    "    mahal_sq = np.sum(left * diff, axis=1)\n",
    "\n",
    "    threshold = chi2.ppf(1 - alpha, df=len(columns))\n",
    "    mask = pd.Series(mahal_sq > threshold, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [Mahalanobis] - Detected {mask.sum()} multivariate outliers (alpha = {alpha})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [Mahalanobis] - No multivariate outliers detected (alpha = {alpha})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply on all numeric columns\n",
    "mahalanobis_mask = detect_outliers_mahalanobis(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdb0f5",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db21a42",
   "metadata": {},
   "source": [
    "<a id=\"ml-methods\"></a>\n",
    "# ğŸ¤– ML-Based Detection Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c581f296",
   "metadata": {},
   "source": [
    "<a id=\"isolation-forest\"></a>\n",
    "#### ğŸŒ² Isolation Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48946b1e",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸŒ² What is Isolation Forest?\n",
    "\n",
    "Isolation Forest is an ensemble-based anomaly detection method that works by **isolating observations** using random splits.  \n",
    "Outliers are easier to isolate and require fewer splits.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Builds multiple trees by randomly selecting features and split values\n",
    "- Each sample's **average path length** across trees is computed\n",
    "- Outliers have shorter path lengths (isolated faster)\n",
    "- Scores are assigned to rank outlier likelihood\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- On **high-dimensional** or large datasets\n",
    "- When a **model-based**, non-parametric detector is preferred\n",
    "- Suitable for both univariate and multivariate outlier detection\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Scales well to large datasets\n",
    "- No distributional assumptions\n",
    "- Handles multivariate relationships\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Output scores are **relative**, not probabilistic\n",
    "- Performance can vary with small datasets\n",
    "- May struggle with highly imbalanced feature importance\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2980bfbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [Isolation Forest] - Detected 5 outliers (contamination = 0.05)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashrithreddy/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but IsolationForest was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def detect_outliers_isolation_forest(df, columns, contamination=0.05, random_state=42):\n",
    "    \"\"\"\n",
    "    Detect outliers using Isolation Forest.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply the method on.\n",
    "        contamination (float): Expected proportion of outliers.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    iso = IsolationForest(contamination=contamination, random_state=random_state)\n",
    "    preds = iso.fit_predict(data)\n",
    "\n",
    "    mask = pd.Series(preds == -1, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [Isolation Forest] - Detected {mask.sum()} outliers (contamination = {contamination})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [Isolation Forest] - No outliers detected (contamination = {contamination})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply Isolation Forest on all numeric columns\n",
    "isoforest_mask = detect_outliers_isolation_forest(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28600b4c",
   "metadata": {},
   "source": [
    "<a id=\"lof\"></a>\n",
    "#### ğŸ” Local Outlier Factor (LOF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1150c404",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ” What is LOF?\n",
    "\n",
    "Local Outlier Factor (LOF) identifies outliers by comparing the **local density** of each point to that of its neighbors.  \n",
    "Outliers have significantly lower density than surrounding points.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- For each point, compute its k-nearest neighbors\n",
    "- Estimate local density based on average distance to those neighbors\n",
    "- Compute the **LOF score** by comparing a pointâ€™s density to that of its neighbors\n",
    "- Higher LOF score = more likely to be an outlier\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- Detecting **local** anomalies that deviate within a neighborhood\n",
    "- Datasets with clusters of varying density\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Captures **local context**, unlike global methods\n",
    "- Good for complex, non-linear distributions\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Sensitive to the choice of `k` (number of neighbors)\n",
    "- Struggles with high-dimensional data\n",
    "- Scores are relative â€” not probability-based\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25cd1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [LOF] - Detected 5 outliers (n_neighbors = 20, contamination = 0.05)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "def detect_outliers_lof(df, columns, n_neighbors=20, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect outliers using Local Outlier Factor (LOF).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply the method on.\n",
    "        n_neighbors (int): Number of neighbors to use for LOF.\n",
    "        contamination (float): Proportion of expected outliers.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    preds = lof.fit_predict(data)\n",
    "\n",
    "    mask = pd.Series(preds == -1, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [LOF] - Detected {mask.sum()} outliers (n_neighbors = {n_neighbors}, contamination = {contamination})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [LOF] - No outliers detected (n_neighbors = {n_neighbors})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply LOF on all numeric columns\n",
    "lof_mask = detect_outliers_lof(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e4a68",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d37a5",
   "metadata": {},
   "source": [
    "<a id=\"proximity-methods\"></a>\n",
    "# ğŸ“ Proximity & Clustering-Based Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2eb090",
   "metadata": {},
   "source": [
    "<a id=\"dbscan\"></a>\n",
    "#### ğŸŒ€ DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36257cbb",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸŒ€ What is DBSCAN?\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies outliers as points that **do not belong to any dense region**.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Groups points into clusters based on density (minPts within a radius Îµ)\n",
    "- Points not reachable from any cluster are labeled as **noise**\n",
    "- These noise points are treated as outliers\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When the data has **irregular shapes or density-based clusters**\n",
    "- Works well for **unsupervised anomaly detection**\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- No need to specify number of clusters\n",
    "- Can detect outliers **as a byproduct** of clustering\n",
    "- Handles arbitrarily shaped clusters\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Requires tuning of Îµ and minPts\n",
    "- Sensitive to scale of features â€” needs normalization\n",
    "- Can fail with **varying density** clusters\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ba272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [DBSCAN] - Detected 32 outliers (eps = 0.5, min_samples = 5)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def detect_outliers_dbscan(df, columns, eps=0.5, min_samples=5):\n",
    "    \"\"\"\n",
    "    Detect outliers using DBSCAN clustering (label -1 as noise).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply DBSCAN on.\n",
    "        eps (float): Maximum distance between samples to be considered neighbors.\n",
    "        min_samples (int): Minimum samples required to form a dense region.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    scaled = StandardScaler().fit_transform(data)\n",
    "\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(scaled)\n",
    "\n",
    "    mask = pd.Series(labels == -1, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [DBSCAN] - Detected {mask.sum()} outliers (eps = {eps}, min_samples = {min_samples})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [DBSCAN] - No outliers detected (eps = {eps}, min_samples = {min_samples})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply DBSCAN on all numeric columns\n",
    "dbscan_mask = detect_outliers_dbscan(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5d9f9",
   "metadata": {},
   "source": [
    "<a id=\"kmeans-distance\"></a>\n",
    "#### ğŸ“ K-Means Distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f36b24b",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“ What is K-Means Distance for Outlier Detection?\n",
    "\n",
    "In K-Means, outliers are often identified as points that are **far from their assigned cluster centroids** â€” i.e., they have high intra-cluster distances.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Run K-Means clustering on the dataset\n",
    "- For each point, compute the distance to its assigned cluster centroid\n",
    "- Rank points by distance; outliers lie in the **tail of the distance distribution**\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- As a **quick unsupervised heuristic**\n",
    "- When K-Means is already used for segmentation and you want to flag edge cases\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Simple to implement using existing clustering output\n",
    "- Scales well to large datasets\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Assumes **spherical clusters** â€” poor performance on non-globular data\n",
    "- Requires setting `k` (number of clusters)\n",
    "- Sensitive to initial centroid selection and feature scaling\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84f76727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [KMeans Distance] - Detected 1 outliers (above 99th percentile distance)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashrithreddy/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "def detect_outliers_kmeans(df, columns, n_clusters=3, threshold_quantile=0.99, random_state=42):\n",
    "    \"\"\"\n",
    "    Detect outliers based on distance to KMeans cluster centroids.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply KMeans on.\n",
    "        n_clusters (int): Number of clusters.\n",
    "        threshold_quantile (float): Quantile of distance above which to flag as outlier.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    scaled = StandardScaler().fit_transform(data)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(scaled)\n",
    "\n",
    "    _, distances = pairwise_distances_argmin_min(scaled, kmeans.cluster_centers_)\n",
    "    threshold = np.quantile(distances, threshold_quantile)\n",
    "    mask = pd.Series(distances > threshold, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [KMeans Distance] - Detected {mask.sum()} outliers (above {int(threshold_quantile * 100)}th percentile distance)\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [KMeans Distance] - No outliers detected (above {int(threshold_quantile * 100)}th percentile distance)\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply KMeans distance-based detection\n",
    "kmeans_mask = detect_outliers_kmeans(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c855e6ee",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31f4a3a",
   "metadata": {},
   "source": [
    "<a id=\"probabilistic-methods\"></a>\n",
    "# ğŸ“ Probabilistic Detection Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8631da5",
   "metadata": {},
   "source": [
    "<a id=\"gmm\"></a>\n",
    "#### ğŸ“Š Gaussian Mixture Models (GMM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb23ef",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“Š What is GMM for Outlier Detection?\n",
    "\n",
    "GMM assumes the data is generated from a mixture of several Gaussian distributions.  \n",
    "Points with **low likelihood** under the fitted model are flagged as outliers.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Fit a Gaussian mixture model using Expectation-Maximization (EM)\n",
    "- Compute the **log-likelihood** of each point under the model\n",
    "- Flag points with likelihood below a threshold as outliers\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- For **continuous multivariate data**\n",
    "- When data clusters resemble Gaussian blobs\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Soft clustering: gives probabilistic interpretation\n",
    "- Handles **complex, multimodal distributions**\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Assumes Gaussian components â€” may fail on heavy-tailed or skewed data\n",
    "- Requires selecting the number of components (can be tuned via BIC/AIC)\n",
    "- Sensitive to outliers during model fitting\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41a64c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [GMM] - Detected 1 outliers (below 1th percentile log-likelihood)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def detect_outliers_gmm(df, columns, n_components=3, threshold_quantile=0.01, random_state=42):\n",
    "    \"\"\"\n",
    "    Detect outliers using Gaussian Mixture Model log-likelihood scores.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply GMM on.\n",
    "        n_components (int): Number of Gaussian components.\n",
    "        threshold_quantile (float): Lower quantile of log-likelihood to flag as outlier.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    data = df[columns].dropna()\n",
    "    scaled = StandardScaler().fit_transform(data)\n",
    "\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=random_state)\n",
    "    gmm.fit(scaled)\n",
    "    log_probs = gmm.score_samples(scaled)\n",
    "\n",
    "    threshold = np.quantile(log_probs, threshold_quantile)\n",
    "    mask = pd.Series(log_probs < threshold, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [GMM] - Detected {mask.sum()} outliers (below {int(threshold_quantile * 100)}th percentile log-likelihood)\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [GMM] - No outliers detected (below {int(threshold_quantile * 100)}th percentile log-likelihood)\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Apply GMM outlier detection\n",
    "gmm_mask = detect_outliers_gmm(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13df6da",
   "metadata": {},
   "source": [
    "<a id=\"evt\"></a>\n",
    "#### ğŸ“‰ Extreme Value Theory (EVT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067d98e6",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“‰ What is Extreme Value Theory (EVT)?\n",
    "\n",
    "Extreme Value Theory models the **tail behavior** of a distribution to identify rare, extreme events.  \n",
    "Itâ€™s often used to detect outliers that lie far in the distribution's tail.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Focuses on modeling the **maximum (or minimum)** values in data\n",
    "- Fits a distribution (e.g., Generalized Pareto) to the tail\n",
    "- Outliers are points with very low probability under this tail distribution\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- In fields where **extreme risks or rare events** are important (e.g., finance, insurance, climate)\n",
    "- When the tail of the distribution carries meaningful signal\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Tail-focused â€” captures true \"extremes\"\n",
    "- Statistically principled for outlier modeling\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Requires sufficient tail data to model\n",
    "- Only effective for **univariate** distributions\n",
    "- Requires careful threshold selection for tail modeling\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "679c3079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mâœ… [feature_1] - No outliers detected using EVT (quantile = 0.95)\u001b[0m\n",
      "\u001b[92mâœ… [feature_2] - No outliers detected using EVT (quantile = 0.95)\u001b[0m\n",
      "\u001b[92mâœ… [feature_3] - No outliers detected using EVT (quantile = 0.95)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def detect_outliers_evt(df, column, threshold_quantile=0.95):\n",
    "    \"\"\"\n",
    "    Detect outliers using Extreme Value Theory (GPD tail modeling).\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Column to apply EVT on.\n",
    "        threshold_quantile (float): Quantile to define tail threshold.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Column '{column}' is either missing or not numeric.\")\n",
    "    \n",
    "    x = df[column].dropna()\n",
    "    threshold = x.quantile(threshold_quantile)\n",
    "    tail_excess = x[x > threshold] - threshold\n",
    "\n",
    "    if len(tail_excess) < 5:\n",
    "        print(f\"\\033[93mâš ï¸ [{column}] - Not enough data in tail for EVT modeling.\\033[0m\")\n",
    "        return pd.Series([False] * len(x), index=x.index)\n",
    "\n",
    "    # Fit GPD to the excess over threshold\n",
    "    c, loc, scale = genpareto.fit(tail_excess)\n",
    "    probs = genpareto.sf(tail_excess, c, loc=loc, scale=scale)\n",
    "\n",
    "    # Build full-length boolean mask\n",
    "    mask = pd.Series(False, index=x.index)\n",
    "    mask.loc[tail_excess.index] = probs < 0.01\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Detected {mask.sum()} outliers using EVT (quantile = {threshold_quantile})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - No outliers detected using EVT (quantile = {threshold_quantile})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Retry EVT detection\n",
    "evt_results = {}\n",
    "for col in df.select_dtypes(include='number').columns:\n",
    "    evt_results[col] = detect_outliers_evt(df, col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8a205",
   "metadata": {},
   "source": [
    "<a id=\"bayesian\"></a>\n",
    "#### ğŸ§  Bayesian Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc5ab3",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ§  What Are Bayesian Methods for Outlier Detection?\n",
    "\n",
    "Bayesian methods model uncertainty in the data and incorporate prior beliefs to estimate the **posterior probability** of a point being an outlier.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Define a generative model with priors over parameters\n",
    "- Use observed data to compute **posterior distributions**\n",
    "- Outliers are identified as points with **low posterior probability** under the model\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When uncertainty modeling is critical\n",
    "- For **probabilistic anomaly detection** in structured or time-series data\n",
    "- When expert knowledge can inform priors\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Explicitly models uncertainty\n",
    "- Flexible and extensible to many domains\n",
    "- Can incorporate prior knowledge\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Computationally intensive (requires sampling or variational inference)\n",
    "- Requires strong modeling assumptions\n",
    "- Interpretation may be non-trivial for high-dimensional data\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284a0a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mâš ï¸ [feature_1] - Detected 2 residual outliers using Bayesian Regression (threshold = 3.0)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_2] - Detected 1 residual outliers using Bayesian Regression (threshold = 3.0)\u001b[0m\n",
      "\u001b[91mâš ï¸ [feature_3] - Detected 2 residual outliers using Bayesian Regression (threshold = 3.0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "def detect_outliers_bayesian_residuals(df, column, features=None, threshold=3.0):\n",
    "    \"\"\"\n",
    "    Detect outliers based on residuals from a Bayesian Ridge regression model.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        column (str): Target column to model and test for outliers.\n",
    "        features (list): Feature columns to use for modeling (default: all others).\n",
    "        threshold (float): Z-score threshold on residuals.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: Boolean mask where True indicates an outlier.\n",
    "    \"\"\"\n",
    "    if column not in df.columns or not np.issubdtype(df[column].dtype, np.number):\n",
    "        raise ValueError(f\"Target column '{column}' is either missing or not numeric.\")\n",
    "    \n",
    "    features = features or [col for col in df.select_dtypes(include='number').columns if col != column]\n",
    "    data = df[[column] + features].dropna()\n",
    "\n",
    "    X = data[features]\n",
    "    y = data[column]\n",
    "\n",
    "    model = BayesianRidge()\n",
    "    model.fit(X, y)\n",
    "    preds = model.predict(X)\n",
    "    residuals = y - preds\n",
    "\n",
    "    z_resid = zscore(residuals)\n",
    "    mask = pd.Series(np.abs(z_resid) > threshold, index=data.index)\n",
    "\n",
    "    if mask.sum() > 0:\n",
    "        print(f\"\\033[91mâš ï¸ [{column}] - Detected {mask.sum()} residual outliers using Bayesian Regression (threshold = {threshold})\\033[0m\")\n",
    "    else:\n",
    "        print(f\"\\033[92mâœ… [{column}] - No residual outliers detected using Bayesian Regression (threshold = {threshold})\\033[0m\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "# Run Bayesian residual outlier detection per column using others as predictors\n",
    "bayes_results = {}\n",
    "numeric_cols = df.select_dtypes(include='number').columns.tolist()\n",
    "for target_col in numeric_cols:\n",
    "    bayes_results[target_col] = detect_outliers_bayesian_residuals(df, target_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4175ba10",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d164bd",
   "metadata": {},
   "source": [
    "<a id=\"treatment-strategies\"></a>\n",
    "# ğŸ› ï¸ Outlier Treatment Strategies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca704e2",
   "metadata": {},
   "source": [
    "<a id=\"deletion\"></a>\n",
    "#### âŒ Deletion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d4ebb6",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### âŒ What is Deletion?\n",
    "\n",
    "Deletion involves simply **removing identified outliers** from the dataset before training or analysis.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Detect outliers using a chosen method (e.g., IQR, Z-Score)\n",
    "- Drop those rows from the dataset using filtering or indexing\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When dataset is **large** and removing a few rows wonâ€™t affect learning\n",
    "- If outliers are clearly **data entry errors** or irrelevant edge cases\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Simple and fast to apply\n",
    "- Removes potential noise from modeling\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Risks **losing valuable signal**, especially in small datasets\n",
    "- Can bias model if outliers carry meaningful variation\n",
    "- Not reversible â€” original data is discarded\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "744ca6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mğŸ”§ Deleted 9 rows containing outliers across any specified column\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def treat_outliers_deletion(df, mask_dict):\n",
    "    \"\"\"\n",
    "    Remove rows from the DataFrame where any of the provided masks are True.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original DataFrame.\n",
    "        mask_dict (dict): Dictionary of {column: boolean mask Series}.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    combined_mask = pd.Series(False, index=df.index)\n",
    "    for col, mask in mask_dict.items():\n",
    "        combined_mask = combined_mask | mask\n",
    "\n",
    "    treated_count = combined_mask.sum()\n",
    "    cleaned_df = df[~combined_mask]\n",
    "\n",
    "    print(f\"\\033[94mğŸ”§ Deleted {treated_count} rows containing outliers across any specified column\\033[0m\")\n",
    "    return cleaned_df\n",
    "\n",
    "# Example: remove all rows detected as outliers by IQR\n",
    "df_deleted = treat_outliers_deletion(df, iqr_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53601287",
   "metadata": {},
   "source": [
    "<a id=\"capping\"></a>\n",
    "#### ğŸ” Capping / Winsorizing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6308488",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ” What is Capping / Winsorizing?\n",
    "\n",
    "Capping (also known as Winsorizing) involves **limiting extreme values** by replacing them with specified percentile thresholds, rather than removing them.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Identify upper and lower percentile cutoffs (e.g., 1st and 99th percentiles)\n",
    "- Replace values above the upper bound with the upper threshold\n",
    "- Replace values below the lower bound with the lower threshold\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When outliers are legitimate but **too influential**\n",
    "- In models sensitive to scale (e.g., linear regression)\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Preserves dataset size and structure\n",
    "- Reduces the influence of extreme values without deletion\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Thresholds are arbitrary â€” may require tuning\n",
    "- Can distort distribution if used aggressively\n",
    "- Doesnâ€™t address multivariate outliers\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a106308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mğŸ”§ Capped 6 outlier values using 1â€“99 percentile thresholds\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def treat_outliers_capping(df, columns, lower_quantile=0.01, upper_quantile=0.99):\n",
    "    \"\"\"\n",
    "    Cap outliers in specified columns based on percentile thresholds.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): Columns to apply capping.\n",
    "        lower_quantile (float): Lower bound percentile.\n",
    "        upper_quantile (float): Upper bound percentile.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with capped values.\n",
    "    \"\"\"\n",
    "    df_capped = df.copy()\n",
    "    total_treated = 0\n",
    "\n",
    "    for col in columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "\n",
    "        lower = df[col].quantile(lower_quantile)\n",
    "        upper = df[col].quantile(upper_quantile)\n",
    "\n",
    "        before = df[col]\n",
    "        capped = before.clip(lower, upper)\n",
    "        treated = (before != capped).sum()\n",
    "        total_treated += treated\n",
    "\n",
    "        df_capped[col] = capped\n",
    "\n",
    "    print(f\"\\033[94mğŸ”§ Capped {total_treated} outlier values using {int(lower_quantile*100)}â€“{int(upper_quantile*100)} percentile thresholds\\033[0m\")\n",
    "    return df_capped\n",
    "\n",
    "# Example: apply capping to all numeric columns\n",
    "df_capped = treat_outliers_capping(df, df.select_dtypes(include='number').columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfb03ee",
   "metadata": {},
   "source": [
    "<a id=\"imputation\"></a>\n",
    "#### ğŸ§® Imputation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ce510",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ§® What is Imputation for Outliers?\n",
    "\n",
    "Imputation replaces outlier values with a more reasonable estimate (e.g., mean, median, or model prediction), treating them similarly to missing data.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Identify outliers using a chosen method\n",
    "- Replace them with:\n",
    "  - **Central tendency** (mean, median)\n",
    "  - **Value from a predictive model** (regression, KNN, etc.)\n",
    "  - **Group-specific statistics** (e.g., median by segment)\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When outliers are suspected to be **corrupted or extreme noise**\n",
    "- When data integrity is important and deletion isn't an option\n",
    "- Especially useful for **time series**, healthcare, or small datasets\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Retains dataset size and row context\n",
    "- Can preserve statistical properties when done carefully\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Imputed values may hide uncertainty\n",
    "- Risk of **biasing** the dataset if imputation method is naive\n",
    "- Not suitable when outliers are meaningful or intentional signals\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa003052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mğŸ”§ Imputed 9 outliers using median strategy\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def treat_outliers_imputation(df, mask_dict, strategy=\"median\"):\n",
    "    \"\"\"\n",
    "    Impute outliers in specified columns using central tendency.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        mask_dict (dict): Dictionary of {column: boolean mask Series}.\n",
    "        strategy (str): Imputation method: 'mean' or 'median'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with outliers imputed.\n",
    "    \"\"\"\n",
    "    df_imputed = df.copy()\n",
    "    total_imputed = 0\n",
    "\n",
    "    for col, mask in mask_dict.items():\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "\n",
    "        if strategy == \"mean\":\n",
    "            value = df[col].mean()\n",
    "        elif strategy == \"median\":\n",
    "            value = df[col].median()\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be 'mean' or 'median'.\")\n",
    "\n",
    "        df_imputed.loc[mask, col] = value\n",
    "        total_imputed += mask.sum()\n",
    "\n",
    "    print(f\"\\033[94mğŸ”§ Imputed {total_imputed} outliers using {strategy} strategy\\033[0m\")\n",
    "    return df_imputed\n",
    "\n",
    "# Example: Impute IQR-detected outliers using median\n",
    "df_imputed = treat_outliers_imputation(df, iqr_results, strategy=\"median\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697044b4",
   "metadata": {},
   "source": [
    "<a id=\"binning\"></a>\n",
    "#### ğŸ“Š Binning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee01a0a",
   "metadata": {},
   "source": [
    "<details><summary><strong>ğŸ“– Click to Expand</strong></summary>\n",
    "\n",
    "##### ğŸ“Š What is Binning?\n",
    "\n",
    "Binning transforms continuous variables into **discrete categories** (bins), which can help smooth out the influence of outliers.\n",
    "\n",
    "##### âš™ï¸ How It Works\n",
    "\n",
    "- Define bin edges manually or using quantiles (equal-width or equal-frequency)\n",
    "- Replace raw values with corresponding bin labels or codes\n",
    "- Outliers naturally fall into **edge bins**, limiting their impact\n",
    "\n",
    "##### ğŸ•µï¸â€â™‚ï¸ When to Use\n",
    "\n",
    "- When interpretability is more important than precision\n",
    "- As a **preprocessing step** for tree-based models or rule-based systems\n",
    "- To mitigate extreme values in skewed distributions\n",
    "\n",
    "##### âœ… Pros\n",
    "\n",
    "- Reduces influence of outliers\n",
    "- Can simplify feature relationships\n",
    "- Useful for feature engineering\n",
    "\n",
    "##### âš ï¸ Limitations\n",
    "\n",
    "- Can lead to **information loss**\n",
    "- Bin choice is arbitrary â€” poor binning can hurt performance\n",
    "- Not suitable for models that rely on continuous features (e.g., linear regression)\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76cbc20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mğŸ”§ Binned 300 values across 3 column(s) using 'quantile' strategy with 4 bins\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def treat_outliers_binning(df, columns, bins=5, strategy=\"quantile\"):\n",
    "    \"\"\"\n",
    "    Discretize numeric columns into bins to reduce outlier impact.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of numeric columns to bin.\n",
    "        bins (int): Number of bins.\n",
    "        strategy (str): 'quantile' or 'uniform'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with binned columns.\n",
    "    \"\"\"\n",
    "    df_binned = df.copy()\n",
    "    total_binned = 0\n",
    "\n",
    "    for col in columns:\n",
    "        if not np.issubdtype(df[col].dtype, np.number):\n",
    "            continue\n",
    "\n",
    "        if strategy == \"quantile\":\n",
    "            df_binned[col], bin_edges = pd.qcut(df[col], q=bins, labels=False, retbins=True, duplicates='drop')\n",
    "        elif strategy == \"uniform\":\n",
    "            df_binned[col], bin_edges = pd.cut(df[col], bins=bins, labels=False, retbins=True)\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be 'quantile' or 'uniform'.\")\n",
    "\n",
    "        total_binned += df[col].notna().sum()\n",
    "\n",
    "    print(f\"\\033[94mğŸ”§ Binned {total_binned} values across {len(columns)} column(s) using '{strategy}' strategy with {bins} bins\\033[0m\")\n",
    "    return df_binned\n",
    "\n",
    "# Example: Apply quantile-based binning to all numeric columns\n",
    "df_binned = treat_outliers_binning(df, df.select_dtypes(include='number').columns.tolist(), bins=4, strategy=\"quantile\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfc7e09",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
