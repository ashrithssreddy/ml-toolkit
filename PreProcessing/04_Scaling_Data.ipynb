{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "- [Standardization](#Standardization)\n",
    "- [Normalization](#Normalization)\n",
    "- [Robust Scaling](#Robust-Scaling)\n",
    "- [MaxAbs Scaling](#MaxAbs-Scaling)\n",
    "- [L1 Normalization](#L1-Normalization)\n",
    "- [L2 Normalization](#L2-Normalization)\n",
    "- [Log Transformation](#Log-Transformation)\n",
    "- [Power Transformation ](#Power-Transformation)\n",
    "- [Clipping](#Clipping)\n",
    "- [Scaling by Range Adjustment](#Scaling-by-Range-Adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "   Feature1  Feature2\n",
      "0       100         1\n",
      "1       200         2\n",
      "2       300         3\n",
      "3       400         4\n",
      "4       500         5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Feature1': [100, 200, 300, 400, 500],\n",
    "    'Feature2': [1, 2, 3, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display original data\n",
    "print(\"Original Data:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization (Z-score Normalization) is a scaling technique where the values are centered around the mean with a unit standard deviation.\n",
    "\n",
    "$$ z = \\frac{x - \\mu}{\\sigma} $$\n",
    "- $x$ is the original feature value,\n",
    "- $\\mu$ is the mean of the feature,\n",
    "- $\\sigma$ is the standard deviation of the feature.\n",
    "\n",
    "Why Use Standardization?\n",
    "- It ensures the data has a mean of 0 and a standard deviation of 1, which is crucial for algorithms sensitive to scale, such as Support Vector Machines (SVM) and Principal Component Analysis (PCA).\n",
    "- Standardization is particularly useful when features have different units or scales.\n",
    "\n",
    "When to Use?\n",
    "- It works well when the dataset has outliers, as it does not squash the values between a fixed range like Min-Max Scaling.\n",
    "- Use it when your machine learning algorithm assumes normally distributed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized Data:\n",
      "   Feature1  Feature2\n",
      "0 -1.414214 -1.414214\n",
      "1 -0.707107 -0.707107\n",
      "2  0.000000  0.000000\n",
      "3  0.707107  0.707107\n",
      "4  1.414214  1.414214\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardization using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df.columns)\n",
    "\n",
    "# Display standardized data\n",
    "print(\"Standardized Data:\")\n",
    "print(scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization (Min-Max Scaling) scales data to a fixed range, typically [0, 1], ensuring all features contribute equally to the model.\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "- $x_{\\text{min}}$: Minimum value of the feature,\n",
    "- $x_{\\text{max}}$: Maximum value of the feature.\n",
    "\n",
    "Advantages:\n",
    "- Preserves the relative relationships between values.\n",
    "- Suitable for algorithms sensitive to feature magnitudes, like k-NN and neural networks.\n",
    "\n",
    "When to Use:\n",
    "- Use when features have varying scales and you want to scale them to a consistent range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Data (Min-Max Scaling):\n",
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create a MinMaxScaler instance\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max Normalization\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "# Display normalized data\n",
    "print(\"Normalized Data (Min-Max Scaling):\")\n",
    "print(normalized_df)\n",
    "\n",
    "\n",
    "# min-max scaler\n",
    "# df[column_name] = (df[column_name] - np.min(df[column_name])) / (np.max(df[column_name]) - np.min(df[column_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Scaling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust Scaling scales data using the median and Interquartile Range (IQR), making it less sensitive to outliers.\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{x - \\text{median}}{\\text{IQR}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Median**: 50th percentile,\n",
    "- **IQR**: 75th percentile (Q3) - 25th percentile (Q1).\n",
    "\n",
    "Advantages:\n",
    "- **Outlier Robust**: Handles extreme values effectively.\n",
    "- Suitable for datasets with outliers.\n",
    "\n",
    "When to Use:\n",
    "- Ideal when your data contains outliers that distort scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0      -1.0      -1.0\n",
      "1      -0.5      -0.5\n",
      "2       0.0       0.0\n",
      "3       0.5       0.5\n",
      "4       1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Create a RobustScaler instance\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Apply Robust Scaling\n",
    "robust_scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "robust_scaled_df = pd.DataFrame(robust_scaled_data, columns=df.columns)\n",
    "\n",
    "# Display robust scaled data\n",
    "print(\"Robust Scaled Data:\")\n",
    "print(robust_scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MaxAbs Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxAbs Scaling scales each feature by its maximum absolute value, transforming the data to a range of `[-1, 1]`. It preserves the sparsity of the data, making it suitable for sparse datasets.\n",
    "\n",
    "$$\n",
    "x_{\\text{scaled}} = \\frac{x}{\\text{max}(|x|)}\n",
    "$$\n",
    "\n",
    "- $x$ is the original feature value,\n",
    "- $\\text{max}(|x|)$ is the maximum absolute value of the feature.\n",
    "\n",
    "Advantages:\n",
    "- **Sparse Data Friendly**: Maintains the sparsity structure of the dataset.\n",
    "- Scales data without shifting the mean.\n",
    "\n",
    "When to Use:\n",
    "- Ideal for datasets with sparse features or when values are already centered at zero.\n",
    "- Useful for models like SVMs or logistic regression with sparse inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxAbs Scaled Data:\n",
      "   Feature1  Feature2\n",
      "0       0.2       0.2\n",
      "1       0.4       0.4\n",
      "2       0.6       0.6\n",
      "3       0.8       0.8\n",
      "4       1.0       1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# Create a MaxAbsScaler instance\n",
    "scaler = MaxAbsScaler()\n",
    "\n",
    "# Apply MaxAbs Scaling\n",
    "maxabs_scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "# Convert the scaled data back to a DataFrame\n",
    "maxabs_scaled_df = pd.DataFrame(maxabs_scaled_data, columns=df.columns)\n",
    "\n",
    "# Display MaxAbs scaled data\n",
    "print(\"MaxAbs Scaled Data:\")\n",
    "print(maxabs_scaled_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 Normalization (`Least Absolute Deviations`) scales each sample (row) individually such that the sum of the absolute values of all features in a sample equals 1. It emphasizes the proportions between feature values rather than their magnitudes.\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x}{\\sum |x|}\n",
    "$$\n",
    "\n",
    "- $x$ is the original feature value,\n",
    "- $\\sum |x|$ is the sum of the absolute values of all features in the sample.\n",
    "\n",
    "Advantages:\n",
    "- Highlights the relative importance of features within each sample.\n",
    "- Ensures all rows (samples) have the same total magnitude.\n",
    "\n",
    "When to Use:\n",
    "- Useful when feature proportions matter more than absolute values, such as in text classification (e.g., term frequency) or histogram-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Create a Normalizer instance with L1 norm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer(norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a Normalizer instance with L1 norm\n",
    "normalizer = Normalizer(norm='l1')\n",
    "\n",
    "# Apply L1 Normalization\n",
    "l1_normalized_data = normalizer.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "l1_normalized_df = pd.DataFrame(l1_normalized_data, columns=df.columns)\n",
    "\n",
    "# Display L1 normalized data\n",
    "print(\"L1 Normalized Data:\")\n",
    "print(l1_normalized_df)\n",
    "\n",
    "# scaling - l1 normalization\n",
    "# df[column_name] = df[column_name] / df[column_name].abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 Normalization (`Euclidean Norm`) scales each sample (row) individually so that the sum of the squared values of all features in a sample equals 1. It ensures that the magnitude (Euclidean norm) of each sample is 1.\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x}{\\sqrt{\\sum x^2}}\n",
    "$$\n",
    "\n",
    "- $x$ is the original feature value,\n",
    "- $\\sqrt{\\sum x^2}$ is the Euclidean norm (L2 norm) of the sample.\n",
    "\n",
    "Advantages:\n",
    "- Preserves the direction of data while normalizing its magnitude.\n",
    "- Useful for machine learning models that rely on feature magnitude and direction, such as SVMs and k-NN.\n",
    "\n",
    "When to Use:\n",
    "- Ideal for datasets where the magnitude of feature vectors should be normalized while retaining their directional information, such as in text data or image feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 Normalized Data:\n",
      "   Feature1  Feature2\n",
      "0   0.99995      0.01\n",
      "1   0.99995      0.01\n",
      "2   0.99995      0.01\n",
      "3   0.99995      0.01\n",
      "4   0.99995      0.01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Create a Normalizer instance with L2 norm\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Apply L2 Normalization\n",
    "l2_normalized_data = normalizer.fit_transform(df)\n",
    "\n",
    "# Convert the normalized data back to a DataFrame\n",
    "l2_normalized_df = pd.DataFrame(l2_normalized_data, columns=df.columns)\n",
    "\n",
    "# Display L2 normalized data\n",
    "print(\"L2 Normalized Data:\")\n",
    "print(l2_normalized_df)\n",
    "\n",
    "\n",
    "# scaling - l2 normalization\n",
    "# df[column_name] = df[column_name] / np.sqrt((df[column_name]**2).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Transformation reduces the impact of large outliers and makes skewed data more normal-like. This implementation provides flexibility to:\n",
    "- Apply log transformation to specific columns.\n",
    "- Choose a custom logarithmic base (e.g., natural log, base 2, base 10).\n",
    "- Add a custom value to the data before applying the log transformation.\n",
    "\n",
    "$$\n",
    "x_{\\text{transformed}} = \\frac{\\log(x + \\text{add})}{\\log(\\text{base})}\n",
    "$$\n",
    "\n",
    "**Columns**: Specify columns for transformation (optional, applies to all columns by default).    \n",
    "**Base**: Set the logarithmic base (e.g., natural log, base 10, or base 2).    \n",
    "**Add**: Add a custom value to the data before applying the logarithm (default is 0).    \n",
    "\n",
    "Advantages:\n",
    "- Handles zeros or negative values by adding a custom constant.\n",
    "- Adaptable to different logarithmic bases for varied use cases.\n",
    "\n",
    "When to Use:\n",
    "- Effective for datasets with skewed distributions, exponential growth, or large outliers.\n",
    "- Adjust the `add` parameter for flexibility with zero or negative data points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Log Transformed Data:\n",
      "   Feature1  Feature2\n",
      "0  2.004321  0.301030\n",
      "1  2.303196  0.477121\n",
      "2  2.478566  0.602060\n",
      "3  2.603144  0.698970\n",
      "4  2.699838  0.778151\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def apply_log_transformation(dataframe, columns=None, base=np.e, add=0):\n",
    "    \"\"\"\n",
    "    Apply log transformation to specific columns of a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): Input DataFrame.\n",
    "        columns (list): List of columns to apply log transformation. If None, apply to all columns.\n",
    "        base (float): Base of the logarithm. Default is natural log (e).\n",
    "        add (float): A value to add to the data before applying log. Default is 0.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with log-transformed values.\n",
    "    \"\"\"\n",
    "    # Select columns for transformation\n",
    "    if columns is None:\n",
    "        columns = dataframe.columns\n",
    "\n",
    "    transformed_data = dataframe.copy()\n",
    "    for col in columns:\n",
    "        transformed_data[col] = np.log(transformed_data[col] + add) / np.log(base)  # Adjust for log base\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "# Example usage\n",
    "# Assuming `df` is the DataFrame to be transformed\n",
    "log_transformed_df = apply_log_transformation(df, columns=['Feature1', 'Feature2'], base=10, add=1)\n",
    "\n",
    "# Display the transformed DataFrame\n",
    "print(\"Generalized Log Transformed Data:\")\n",
    "print(log_transformed_df)\n",
    "\n",
    "# scaling - log transform to the base e\n",
    "# df[column_name] = np.log(df[column_name]) / np.log(math.e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Power Transformation\n",
    "AKA Box-Cox and Yeo-Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power Transformation is used to stabilize variance, reduce skewness, and make data more Gaussian-like. It is particularly useful for skewed datasets.\n",
    "\n",
    "1. **Box-Cox**:\n",
    "   - Requires all input data to be positive.\n",
    "   - Suitable for strictly positive data.\n",
    "2. **Yeo-Johnson**:\n",
    "   - Handles both positive and negative values.\n",
    "\n",
    "Parameters:\n",
    "- **method**: `'box-cox'` or `'yeo-johnson'`.\n",
    "- **standardize**: Whether to center and scale the transformed data (default is `True`).\n",
    "\n",
    "Advantages:\n",
    "- Reduces skewness and stabilizes variance.\n",
    "- Useful for linear regression and other algorithms sensitive to data distribution.\n",
    "\n",
    "When to Use:\n",
    "- Use when features are highly skewed or non-Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power Transformed Data:\n",
      "   Feature1  Feature2\n",
      "0 -1.500778 -1.472976\n",
      "1 -0.647010 -0.669761\n",
      "2  0.078865  0.055343\n",
      "3  0.732301  0.727399\n",
      "4  1.336622  1.359996\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Create a PowerTransformer instance (default is Yeo-Johnson)\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "\n",
    "# Apply Power Transformation\n",
    "power_transformed_data = power_transformer.fit_transform(df)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "power_transformed_df = pd.DataFrame(power_transformed_data, columns=df.columns)\n",
    "\n",
    "# Display Power Transformed Data\n",
    "print(\"Power Transformed Data:\")\n",
    "print(power_transformed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantile Transformation\n",
    "(Rank-Based Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantile Transformation maps data to a uniform or normal distribution by ranking and scaling the values. It ensures all features follow a specified distribution.\n",
    "\n",
    "- **output_distribution**: Distribution to map to (`'uniform'` or `'normal'`).\n",
    "- **random_state**: Ensures reproducibility.\n",
    "\n",
    "Advantages:\n",
    "- Handles skewed or non-linear data effectively.\n",
    "- Ensures uniform or normal distribution for each feature.\n",
    "\n",
    "When to Use:\n",
    "- Use for algorithms sensitive to feature distributions or to mitigate skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile Transformed Data:\n",
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashrithreddy/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:2627: UserWarning: n_quantiles (1000) is greater than the total number of samples (5). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Create a QuantileTransformer instance\n",
    "quantile_transformer = QuantileTransformer(output_distribution='uniform', random_state=42)\n",
    "\n",
    "# Apply Quantile Transformation\n",
    "quantile_transformed_data = quantile_transformer.fit_transform(df)\n",
    "\n",
    "# Convert the transformed data back to a DataFrame\n",
    "quantile_transformed_df = pd.DataFrame(quantile_transformed_data, columns=df.columns)\n",
    "\n",
    "# Display Quantile Transformed Data\n",
    "print(\"Quantile Transformed Data:\")\n",
    "print(quantile_transformed_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clipping is a preprocessing technique that restricts data values within a specified range by capping them at a minimum and/or maximum value. It removes extreme values (outliers) by replacing them with boundary values.\n",
    "\n",
    "For a value $x$:\n",
    "$$\n",
    "x_{\\text{clipped}} =\n",
    "\\begin{cases}\n",
    "\\text{min\\_val}, & \\text{if } x < \\text{min\\_val}, \\\\\n",
    "x, & \\text{if } \\text{min\\_val} \\leq x \\leq \\text{max\\_val}, \\\\\n",
    "\\text{max\\_val}, & \\text{if } x > \\text{max\\_val}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- **min_val**: The minimum threshold. Values below this are set to `min_val`.\n",
    "- **max_val**: The maximum threshold. Values above this are set to `max_val`.\n",
    "\n",
    "Advantages:\n",
    "- Handles extreme outliers by limiting their impact.\n",
    "- Keeps data within a meaningful range.\n",
    "\n",
    "When to Use:\n",
    "- Use when outliers might skew results or are invalid.\n",
    "- Common for datasets with natural limits (e.g., sensor readings).\n",
    "\n",
    "Example:\n",
    "If a dataset has values outside the range [0, 100], clipping will set all values below 0 to 0 and above 100 to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped Data:\n",
      "   Feature1  Feature2\n",
      "0       100         1\n",
      "1       100         2\n",
      "2       100         3\n",
      "3       100         4\n",
      "4       100         5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define thresholds\n",
    "min_val = 0\n",
    "max_val = 100\n",
    "\n",
    "# Apply clipping to the DataFrame\n",
    "clipped_df = df.clip(lower=min_val, upper=max_val)\n",
    "\n",
    "# Display clipped data\n",
    "print(\"Clipped Data:\")\n",
    "print(clipped_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling by Range Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling by range adjustment rescales data to fit within a specified range, typically [0, 1]. Each feature is adjusted proportionally to its range.\n",
    "\n",
    "For a value $x$:\n",
    "$$\n",
    "x_{\\text{scaled}} = \\left( \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}} \\right) \\times (\\text{desired\\_max} - \\text{desired\\_min}) + \\text{desired\\_min}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_{\\text{min}}$ and $x_{\\text{max}}$ are the minimum and maximum values of the feature.\n",
    "\n",
    "\n",
    "#### Advantages:\n",
    "- Scales all values into a consistent range.\n",
    "- Preserves the relative distribution of data.\n",
    "\n",
    "#### When to Use:\n",
    "- Use when features have varying scales, and a consistent range is needed (e.g., neural networks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled by Range Adjustment Data:\n",
      "   Feature1  Feature2\n",
      "0      0.00      0.00\n",
      "1      0.25      0.25\n",
      "2      0.50      0.50\n",
      "3      0.75      0.75\n",
      "4      1.00      1.00\n"
     ]
    }
   ],
   "source": [
    "# Define the desired range\n",
    "desired_min = 0\n",
    "desired_max = 1\n",
    "\n",
    "# Apply range adjustment\n",
    "scaled_by_range_df = (df - df.min()) / (df.max() - df.min()) * (desired_max - desired_min) + desired_min\n",
    "\n",
    "# Display the scaled DataFrame\n",
    "print(\"Scaled by Range Adjustment Data:\")\n",
    "print(scaled_by_range_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "[Back to the top](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "\n",
    "    \n",
    "    # zscore/standardization (variance scaling)\n",
    "    df[column_name] = (df[column_name] - np.mean(df[column_name])) / np.std(df[column_name])        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
