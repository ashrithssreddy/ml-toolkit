{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7946c46b",
   "metadata": {},
   "source": [
    "![Python](https://img.shields.io/badge/python-3.9-blue)\n",
    "![Status: Pending Migration](https://img.shields.io/badge/status-pending%20migration-orange)\n",
    "\n",
    "<a id=\"table-of-contents\"></a>\n",
    "# üìñ Classification\n",
    "\n",
    "[üß≠ Objective](#-objective)\n",
    "- [üìå What is Classification?](#-what-is-classification)\n",
    "- [üì¶ Use Cases](#-use-cases)\n",
    "\n",
    "[üìÇ Data Setup](#-data-setup)\n",
    "- [üì• Load Dataset](#-load-dataset)\n",
    "- [üìä Data Characteristics Dictionary](#-data-characteristics-dictionary)\n",
    "- [üßπ Preprocessing](#-preprocessing)\n",
    "\n",
    "[üß™ Baseline Classifier Model](#-baseline-classifier-model)\n",
    "\n",
    "[üìä Model Evaluation](#-model-evaluation)\n",
    "- [üìâ Confusion Matrix](#-confusion-matrix)\n",
    "- [üìà ROC Curve / AUC](#-roc-curve--auc)\n",
    "- [üìè Precision / Recall / F1](#-precision--recall--f1)\n",
    "\n",
    "[üîç Models](#-models)\n",
    "- [üìä Logistic Regression](#-logistic-regression)\n",
    "- [üßÆ Naive Bayes](#-naive-bayes)\n",
    "- [üå≥ Decision Tree](#-decision-tree)\n",
    "- [üå≤ Random Forest](#-random-forest)\n",
    "- [üéØ KNN (K-Nearest Neighbors)](#-knn-k-nearest-neighbors)\n",
    "- [üìà SVM (Support Vector Machines)](#-svm-support-vector-machines)\n",
    "- [üöÄ XGBoost](#-xgboost)\n",
    "- [üß† Neural Network](#-neural-network)\n",
    "\n",
    "[üìä Model Exploration](#-model-exploration)\n",
    "- [üìà Model Comparison](#-model-comparison)\n",
    "- [üìä Feature Importance](#-feature-importance)\n",
    "- [üß¨ SHAP Values](#-shap-values)\n",
    "\n",
    "[üõ†Ô∏è Fine-Tune the Winner](#-fine-tune-the-winner)\n",
    "- [üîé Grid Search](#-grid-search)\n",
    "- [üé≤ Randomized Search](#-randomized-search)\n",
    "\n",
    "[üîÄ Ensemble Methods](#-ensemble-methods)\n",
    "- [üó≥Ô∏è Voting Classifier](#-voting-classifier)\n",
    "- [üß¨ Stacking Classifier](#-stacking-classifier)\n",
    "- [ü™µ Bagging](#-bagging)\n",
    "- [üöÄ Boosting](#-boosting)\n",
    "- [üì¶ Export & Deployment (Optional)](#-export--deployment-optional)\n",
    "\n",
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6e3d2",
   "metadata": {},
   "source": [
    "<a id=\"objective\"></a>\n",
    "# üß≠ Objective\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafdd70",
   "metadata": {},
   "source": [
    "<a id=\"what-is-classification\"></a>\n",
    "#### üìå What is Classification?\n",
    "\n",
    "<details><summary><strong>üìñ Click to Expand</strong></summary>\n",
    "Classification is a type of supervised machine learning where the goal is to predict a categorical label for an observation. Given a set of features (input data), the model tries to assign the observation to one of several predefined classes. Common examples of classification problems include:\n",
    "- **Spam detection**: Classifying emails as spam or not.\n",
    "- **Customer churn prediction**: Classifying customers as likely to leave (churn) or stay based on their activity.\n",
    "- **Image recognition**: Classifying images into categories, like identifying animals, vehicles, etc.\n",
    "\n",
    "In classification, the output is discrete (e.g., 'spam' vs 'not spam', 'churn' vs 'no churn'). This contrasts with regression, where the output is continuous (e.g., predicting a house price).\n",
    "\n",
    "##### Key Points\n",
    "- Supervised learning approach.\n",
    "- Used for predicting categories.\n",
    "- Output is discrete (binary or multiclass).\n",
    "- Examples: email classification, disease diagnosis, fraud detection.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8616d21",
   "metadata": {},
   "source": [
    "<a id=\"classification-use-cases\"></a>\n",
    "#### üì¶ Use Cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4415e",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f37c7",
   "metadata": {},
   "source": [
    "<a id=\"data-setup\"></a>\n",
    "# üìÇ Data Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09beb6",
   "metadata": {},
   "source": [
    "<a id=\"load-dataset\"></a>\n",
    "#### üì• Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c540ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning and Model Evaluation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Statistical and Other Utilities\n",
    "from scipy.stats import zscore\n",
    "from termcolor import colored\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf2c74",
   "metadata": {},
   "source": [
    "<details><summary><strong>üìñ Click to Expand</strong></summary>\n",
    "In this section, we will begin by preparing the dataset. For simplicity, we'll use a simulated classification dataset generated using the `make_classification` function from `sklearn`. This allows us to create a synthetic dataset that is suitable for practicing classification tasks.\n",
    "\n",
    "We will simulate a dataset with the following properties:\n",
    "- 1000 samples (observations)\n",
    "- 10 features (predictors)\n",
    "- 2 informative features (ones that help in prediction)\n",
    "- 2 classes (binary classification problem)\n",
    "\n",
    "Let's generate and take a look at the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate base classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=2,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],  # simulate class imbalance\n",
    "    flip_y=0.01,         # 1% label noise\n",
    "    class_sep=0.8,       # less separation = harder task\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 11)])\n",
    "target_col = \"Target\" \n",
    "df[target_col] = y\n",
    "\n",
    "# Inject missing values randomly (e.g., 1% of cells)\n",
    "# mask = np.random.rand(*df.shape) < 0.01\n",
    "# df[mask] = np.nan\n",
    "\n",
    "# Display preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28407bd",
   "metadata": {},
   "source": [
    "<a id=\"data-characteristics-dictionary\"></a>\n",
    "\n",
    "#### üìä Data Characteristics Dictionary\n",
    "\n",
    "<details><summary><strong>üìñ Click to Expand Explanation</strong></summary>\n",
    "\n",
    "This section initializes the **data characteristics dictionary**, which will store various metadata about the dataset, including details about the target variable, features, data size, and linear separability.\n",
    "\n",
    "The dictionary contains the following key sections:\n",
    "\n",
    "1. **üéØ Target Variable**:\n",
    "   - **Type**: Specifies whether the target variable is **binary** or **multiclass**.\n",
    "   - **Imbalance**: Indicates whether the target variable has **class imbalance**.\n",
    "   - **Class Imbalance Severity**: Specifies the severity of the imbalance (e.g., **high**, **low**).\n",
    "\n",
    "2. **üîß Features**:\n",
    "   - **Type**: Describes the type of features in the dataset (e.g., **categorical**, **continuous**, or **mixed**).\n",
    "   - **Correlation**: Indicates the correlation between features (e.g., **low**, **medium**, **high**).\n",
    "   - **Outliers**: Flag to indicate whether **outliers** are detected in the features.\n",
    "   - **Missing Data**: Tracks the percentage of **missing data** or flags missing values.\n",
    "\n",
    "3. **üìà Data Size**:\n",
    "   - **Size**: Contains the **number of samples** (rows) and **number of features** (columns).\n",
    "\n",
    "4. **üîç Linear Separability**:\n",
    "   - **Linear Separability**: States whether the classes are **linearly separable** (True or False).\n",
    "\n",
    "This dictionary will be updated dynamically as we analyze the dataset in subsequent steps. It serves as a **summary of key dataset properties** to help guide further analysis and modeling decisions.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73753c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data characteristics dictionary\n",
    "data_characteristics = {\n",
    "    \"target_variable\": {\n",
    "        \"type\": None,  # \"binary\", \"multiclass\"\n",
    "        \"imbalance\": None,  # True if imbalanced, False otherwise\n",
    "        \"class_imbalance_severity\": None  # e.g., \"high\", \"low\"\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"type\": None,  # \"categorical\", \"continuous\", \"mixed\"\n",
    "        \"correlation\": None,  # \"low\", \"medium\", \"high\"\n",
    "        \"outliers\": None,  # True if outliers detected, False otherwise\n",
    "        \"missing_data\": None  # Percentage of missing data or boolean\n",
    "    },\n",
    "    \"data_size\": None,  # Size of dataset (samples, features)\n",
    "    \"linear_separability\": None  # True if classes are linearly separable\n",
    "}\n",
    "\n",
    "model_results = {}  # model performance dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffb3b8",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "#### üßπ Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=target_col)\n",
    "y = df[target_col]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data split complete:\")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77810fb9",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be293f5",
   "metadata": {},
   "source": [
    "<a id=\"baseline-model\"></a>\n",
    "# üß™ Baseline Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d9e15",
   "metadata": {},
   "source": [
    "<details><summary><strong>üìñ Click to Expand </strong></summary>\n",
    "\n",
    "In this section, we define the **baseline model** for the classification task. The baseline model is typically a **dummy model** that can be used to compare against more sophisticated models. Here, we use the **DummyClassifier**, which predicts the majority class, to set a baseline performance.\n",
    "\n",
    "The baseline model will help us assess if more advanced models (e.g., Random Forest, SVM) are making meaningful improvements over a simple strategy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Fit a dummy classifier as a baseline\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")  # or try \"stratified\", \"uniform\"\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fe3d2",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "# üìä Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780d645",
   "metadata": {},
   "source": [
    "<details><summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "- **Accuracy**: Overall correctness. Misleading when classes are imbalanced.\n",
    "- **Precision**: Of predicted positives, how many are truly positive? Important when false positives are costly.\n",
    "- **Recall**: Of actual positives, how many did we catch? Crucial when missing positives is expensive.\n",
    "- **F1 Score**: Harmonic mean of precision and recall. Useful when you care about balance.\n",
    "- **ROC AUC**: Probability a random positive ranks above a random negative. Good for probability-based classifiers.\n",
    "\n",
    "We'll log all metrics per model to enable direct comparisons during tuning or model selection.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d488821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Technical output\n",
    "print(\"üìâ Technical Output (Classification Report)\\n\")\n",
    "print(classification_report(y_test, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine positive class label\n",
    "positive_class = y_train.unique()[1] if len(y_train.unique()) == 2 else 1\n",
    "\n",
    "# Compute metrics\n",
    "acc  = accuracy_score(y_test, y_pred_dummy)\n",
    "prec = precision_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "rec  = recall_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "f1   = f1_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "\n",
    "# Print with padded % values to align arrows\n",
    "print(\"\\nüìä Business-Friendly Summary:\")\n",
    "print(f\"- Accuracy  : {acc :>7.2%} ‚Üí Overall correctness. How many total predictions were right.\")\n",
    "print(f\"- Precision : {prec:>7.2%} ‚Üí Of the cases we predicted as '{positive_class}', how many were actually '{positive_class}'.\")\n",
    "print(f\"- Recall    : {rec :>7.2%} ‚Üí Of all the actual '{positive_class}' cases, how many did we correctly identify.\")\n",
    "print(f\"- F1 Score  : {f1  :>7.2%} ‚Üí A balance between Precision and Recall. Higher means more reliable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b6cba",
   "metadata": {},
   "source": [
    "<a id=\"confusion-matrix\"></a>\n",
    "#### üìâ Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1360e6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "The confusion matrix is a 2x2 table that helps us visualize the performance of a classification model.  \n",
    "Each cell represents the count of:\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive cases  \n",
    "- **False Positives (FP)**: Incorrectly predicted as positive  \n",
    "- **True Negatives (TN)**: Correctly predicted negative cases  \n",
    "- **False Negatives (FN)**: Missed positive cases (predicted as negative)\n",
    "\n",
    "To make interpretation easier, we also show **percentages** alongside the raw counts.\n",
    "\n",
    "We use this baseline matrix to see how well future models improve over this simple benchmark.\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with both count and percentage annotations.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm)\n",
    "    cm_perc = cm / cm_sum * 100\n",
    "\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            annot[i, j] = f\"{c}\\n({p:.1f}%)\"\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=annot, fmt=\"\", cmap=\"Blues\", cbar=True,\n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(f\"Confusion Matrix ({model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(y_test, y_pred_dummy, model_name=\"Baseline Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fd3c7",
   "metadata": {},
   "source": [
    "<a id=\"roc-auc\"></a>\n",
    "#### üìà ROC Curve / AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ca9c4",
   "metadata": {},
   "source": [
    "<details><summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "**ROC Curve** (Receiver Operating Characteristic) plots the True Positive Rate (TPR) vs False Positive Rate (FPR) across different threshold values.\n",
    "\n",
    "- A model that randomly guesses would fall along the diagonal (AUC = 0.5)\n",
    "- A perfect model hugs the top-left corner (AUC = 1.0)\n",
    "\n",
    "**AUC (Area Under the Curve)** quantifies overall separability between the two classes:\n",
    "- **Technical Insight**: Higher AUC means better discrimination between positive and negative cases.\n",
    "- **Business Relevance**: Especially useful when false positives and false negatives have different costs ‚Äî like fraud detection, churn prediction, etc.\n",
    "\n",
    "This plot lets stakeholders quickly gauge how good the model is ‚Äî regardless of classification threshold.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_auc(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve, print AUC score, and give business-facing interpretation.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support probability estimates or decision function.\")\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    auc_score = roc_auc_score(y_test, y_scores)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Output\n",
    "    print(f\"üîπ ROC AUC Score for {model_name}: {auc_score:.4f}\")\n",
    "    if auc_score <= 0.55:\n",
    "        print(\"üìå Interpretation: Model performs at or near random. It cannot meaningfully separate classes.\")\n",
    "    elif auc_score < 0.7:\n",
    "        print(\"üìå Interpretation: Some separability, but not reliable yet. Needs improvement.\")\n",
    "    else:\n",
    "        print(\"üìå Interpretation: Model is doing a good job distinguishing between classes.\")\n",
    "\n",
    "plot_roc_auc(dummy_clf, X_test, y_test, model_name=\"Baseline Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deee67",
   "metadata": {},
   "source": [
    "<a id=\"prf-metrics\"></a>\n",
    "#### üìè Precision / Recall / F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741aba9",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "**Precision, Recall, and F1 Score** are classification metrics that help us understand model performance beyond just accuracy:\n",
    "\n",
    "- **Precision**: Of all predicted positives, how many were actually correct? (Low precision = many false alarms)\n",
    "- **Recall**: Of all actual positives, how many did we catch? (Low recall = missed positives)\n",
    "- **F1 Score**: Harmonic mean of precision and recall ‚Äî useful when classes are imbalanced.\n",
    "\n",
    "**Business Perspective**:\n",
    "- If false positives are costly (e.g., spam filters, fraud flags), precision matters more.\n",
    "- If missing positives is risky (e.g., cancer detection), recall is critical.\n",
    "- F1 balances both and gives a single, interpretable metric.\n",
    "\n",
    "These metrics are vital when accuracy is misleading ‚Äî especially in skewed datasets.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaae3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def print_prf_metrics(y_true, y_pred, model_name=\"Model\", average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Print precision, recall, and F1 score with both technical and business-friendly outputs.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Technical metrics\n",
    "    print(f\"üîπ Precision: {precision:.4f}\")\n",
    "    print(f\"üîπ Recall:    {recall:.4f}\")\n",
    "    print(f\"üîπ F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    # Business interpretation\n",
    "    print(\"\\nüìå Interpretation:\")\n",
    "    if precision < 0.6:\n",
    "        print(\"- Model may generate too many false positives ‚Äî not ideal where false alarms are costly.\")\n",
    "    else:\n",
    "        print(\"- Precision looks acceptable; false positives are relatively under control.\")\n",
    "\n",
    "    if recall < 0.6:\n",
    "        print(\"- Model is missing many actual positives ‚Äî risky if false negatives have high cost.\")\n",
    "    else:\n",
    "        print(\"- Recall is strong; model is catching most of the true cases.\")\n",
    "\n",
    "    print(f\"- F1 Score indicates overall balance between precision and recall: {f1:.2f}\")\n",
    "\n",
    "print_prf_metrics(y_test, y_pred_dummy, model_name=\"Baseline Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd53b18",
   "metadata": {},
   "source": [
    "\n",
    "[Back to the top](#table-of-contents)\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb56812",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "# üîç Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b01510",
   "metadata": {},
   "source": [
    "<a id=\"logistic-regression\"></a>\n",
    "#### üìä Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dcdb4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is Logistic Regression?\n",
    "\n",
    "Despite the name, **Logistic Regression** is used for classification ‚Äî not regression.  \n",
    "It predicts the **probability** that an observation belongs to a certain class (e.g., 0 or 1).  \n",
    "Under the hood, it fits a weighted formula to the input features, applies a sigmoid function, and outputs a value between 0 and 1.\n",
    "\n",
    "> Example:  \n",
    "> A model might say there's a **78% chance** this customer will churn.  \n",
    "> If that crosses a certain threshold (say, 50%), we classify it as ‚ÄúYes.‚Äù\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                              | Cons                                  |\n",
    "|-----------------------------------|---------------------------------------|\n",
    "| Fast and efficient                | Assumes linear relationship (log-odds) |\n",
    "| Easy to interpret (feature weights) | Doesn‚Äôt handle complex patterns well  |\n",
    "| Works well with small datasets    | Sensitive to multicollinearity        |\n",
    "| Outputs probabilities             | May underperform on nonlinear data    |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use Logistic Regression when:\n",
    "- You want a **quick baseline** with interpretable output\n",
    "- You care about **probabilities**, not just labels\n",
    "- Your data is fairly **linearly separable**\n",
    "- The number of features is small to medium\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: If features are highly correlated (multicollinearity), the model may become unstable. Use regularization (e.g., L2 penalty).\n",
    "- **Hack**: For imbalanced datasets, adjust the threshold or use `class_weight='balanced'` to avoid bias toward the majority class.\n",
    "- **Tip**: Standardize features before training, especially if using regularization.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680be07",
   "metadata": {},
   "source": [
    "<a id=\"naive-bayes\"></a>\n",
    "#### üßÆ Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c52f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is a family of **probabilistic classifiers** based on Bayes‚Äô Theorem.  \n",
    "It assumes that all features are **independent** of each other ‚Äî which is rarely true in practice, but the model still performs surprisingly well.\n",
    "\n",
    "It calculates the probability of each class given the input features and picks the class with the highest likelihood.\n",
    "\n",
    "> Example:  \n",
    "> ‚ÄúGiven these symptoms, what‚Äôs the most probable disease?‚Äù ‚Äî Naive Bayes is widely used in text classification, spam detection, and medical diagnosis.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                               | Cons                                  |\n",
    "|------------------------------------|---------------------------------------|\n",
    "| Very fast and scalable             | Assumes feature independence (naive)  |\n",
    "| Handles high-dimensional data well | May underperform with correlated inputs |\n",
    "| Simple and interpretable           | Struggles with numeric feature scaling |\n",
    "| Works well with text data          | Outputs are often overconfident       |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use Naive Bayes when:\n",
    "- You‚Äôre working with **text** (e.g., spam filters, sentiment)\n",
    "- You want a **fast baseline**\n",
    "- You‚Äôre dealing with **high-dimensional**, sparse features (like TF-IDF)\n",
    "- You have clean categorical or binary features\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Doesn‚Äôt handle continuous features naturally ‚Äî convert them to bins or use GaussianNB.\n",
    "- **Hack**: Apply **Laplace smoothing** to handle zero probabilities in unseen combinations.\n",
    "- **Tip**: Don‚Äôt expect high accuracy on raw numeric data ‚Äî it shines in text-like scenarios.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a831589",
   "metadata": {},
   "source": [
    "<a id=\"decision-tree\"></a>\n",
    "#### üå≥ Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279d386",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is a Decision Tree?\n",
    "\n",
    "A Decision Tree splits data into branches based on feature values, creating a flowchart-like structure.  \n",
    "Each split is chosen to maximize class separation (typically using Gini impurity or entropy).  \n",
    "The result is a set of human-readable rules ‚Äî like:  \n",
    "‚ÄúIf age < 30 and income > 50K ‚Üí likely to churn.‚Äù\n",
    "\n",
    "It‚Äôs intuitive and easy to explain, even to non-technical stakeholders.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                             | Cons                                 |\n",
    "|----------------------------------|--------------------------------------|\n",
    "| Easy to visualize and interpret  | Prone to overfitting on noisy data   |\n",
    "| No need for feature scaling      | Can create unstable splits           |\n",
    "| Captures non-linear relationships | Doesn‚Äôt generalize well on small data |\n",
    "| Works for both numeric and categorical | Can be biased toward dominant features |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use Decision Trees when:\n",
    "- You need a model that‚Äôs **explainable** (e.g., in regulated domains)\n",
    "- Your data has **mixed types** (numeric + categorical)\n",
    "- You want to **prototype quickly** and understand feature importance\n",
    "- You‚Äôre okay with less predictive power in favor of interpretability\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Deep trees can memorize the training data ‚Äî always prune or set `max_depth`.\n",
    "- **Hack**: Use as a weak learner inside ensembles (like Random Forest or XGBoost) to improve performance.\n",
    "- **Tip**: Use feature importance from trees to guide feature selection for other models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad987ee3",
   "metadata": {},
   "source": [
    "<a id=\"random-forest\"></a>\n",
    "#### üå≤ Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ca9ca",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is a Random Forest?\n",
    "\n",
    "Random Forest is an **ensemble method** that builds many decision trees and combines their outputs.  \n",
    "Each tree sees a random subset of the data and features, making the forest **diverse and robust**.\n",
    "\n",
    "It works by aggregating the predictions of multiple trees (majority vote for classification), reducing the overfitting risk of a single decision tree.\n",
    "\n",
    "> Think of it as a crowd of weak models working together to make better predictions.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                                  | Cons                                   |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Strong performance out of the box     | Less interpretable than a single tree  |\n",
    "| Handles non-linearities and interactions | Slower for real-time predictions     |\n",
    "| Resistant to overfitting              | May require tuning to perform well     |\n",
    "| Works well with large feature spaces  | Not ideal when interpretability is key |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use Random Forest when:\n",
    "- You need a **reliable general-purpose model** with minimal tuning\n",
    "- You want to **improve stability** over a single decision tree\n",
    "- Your data is **tabular and structured**\n",
    "- You care more about **performance** than full interpretability\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: May become large and slow ‚Äî tune `n_estimators` and `max_depth` if needed\n",
    "- **Hack**: Use `feature_importances_` to find influential variables\n",
    "- **Tip**: Avoid one-hot encoding with high-cardinality features ‚Äî use label encoding instead\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a4b23",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "#### üéØ KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e97a2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is K-Nearest Neighbors?\n",
    "\n",
    "KNN is a **non-parametric**, instance-based learning method.  \n",
    "It doesn‚Äôt learn a model during training ‚Äî instead, it stores the data.  \n",
    "At prediction time, it looks at the **K most similar observations** (neighbors) and assigns the class based on majority vote.\n",
    "\n",
    "Similarity is usually measured using **Euclidean distance** (or other distance metrics for different data types).\n",
    "\n",
    "> Example:  \n",
    "> ‚ÄúTo predict a label for this point, look at its 5 closest data points and choose the most common class.‚Äù\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                                 | Cons                                      |\n",
    "|--------------------------------------|-------------------------------------------|\n",
    "| Simple and intuitive                 | Slow at prediction time (no training step) |\n",
    "| No training required                 | Struggles with high-dimensional data       |\n",
    "| Captures local patterns              | Requires feature scaling                  |\n",
    "| Flexible distance metrics            | Memory-intensive with large datasets      |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use KNN when:\n",
    "- You have **low-dimensional**, clean data\n",
    "- You want to **prototype quickly** with minimal assumptions\n",
    "- You care about **local behavior** rather than global rules\n",
    "- Interpretability is less important than flexibility\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Distance metrics break down in high-dimensional space (curse of dimensionality)\n",
    "- **Hack**: Use `StandardScaler` or `MinMaxScaler` to normalize features before fitting\n",
    "- **Tip**: Tune `k` using cross-validation; odd numbers help avoid ties in binary classification\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550a922",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>\n",
    "#### üìà SVM (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbd637",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is SVM?\n",
    "\n",
    "Support Vector Machines (SVM) are **margin-based classifiers** that try to find the best boundary (hyperplane) that separates classes.  \n",
    "SVM focuses on **support vectors** ‚Äî the critical data points closest to the boundary ‚Äî to maximize the margin between classes.\n",
    "\n",
    "It can handle **non-linear patterns** using kernel tricks (e.g., RBF kernel), making it flexible for complex data.\n",
    "\n",
    "> Think of it as drawing the widest possible gap between two classes while avoiding overlap.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                                 | Cons                                   |\n",
    "|--------------------------------------|----------------------------------------|\n",
    "| Works well in high-dimensional spaces | Slow on large datasets                 |\n",
    "| Effective for non-linear boundaries   | Requires careful parameter tuning      |\n",
    "| Robust to overfitting (with regularization) | Not intuitive to interpret         |\n",
    "| Supports different kernels            | Doesn‚Äôt scale well with noisy data     |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use SVM when:\n",
    "- Your data is **high-dimensional**, but you want a **non-linear model**\n",
    "- You need a **strong classifier** and have time to tune hyperparameters\n",
    "- Dataset is **moderate in size** and reasonably clean\n",
    "- You care about maximizing **margin of separation**\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Doesn't output probabilities by default ‚Äî use `probability=True` in `SVC` if needed\n",
    "- **Hack**: Use **RBF kernel** as a good starting point for non-linear problems\n",
    "- **Tip**: Always standardize features ‚Äî SVM is sensitive to feature scale\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb6069",
   "metadata": {},
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "#### üöÄ XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2f79e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is XGBoost?\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful **boosted tree ensemble** method.  \n",
    "Unlike Random Forest (which builds trees in parallel), XGBoost builds trees **sequentially** ‚Äî each new tree tries to fix the errors of the previous one.\n",
    "\n",
    "It uses **gradient descent** to minimize loss, with regularization to prevent overfitting.  \n",
    "XGBoost is known for its **speed, accuracy, and efficiency**, making it a go-to model in many Kaggle competitions and production systems.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                                 | Cons                                   |\n",
    "|--------------------------------------|----------------------------------------|\n",
    "| High predictive accuracy             | Harder to interpret                    |\n",
    "| Built-in regularization (less overfitting) | More complex than basic tree models |\n",
    "| Fast and scalable                    | Requires tuning for best performance   |\n",
    "| Handles missing data automatically   | May overfit small/noisy datasets       |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use XGBoost when:\n",
    "- You need **top-tier performance** on structured/tabular data\n",
    "- You‚Äôre working with **noisy or complex relationships**\n",
    "- You‚Äôre okay with a black-box model in exchange for results\n",
    "- You want built-in tools for **feature importance**, early stopping, etc.\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Easy to overfit if `n_estimators` is too high ‚Äî always monitor with validation\n",
    "- **Hack**: Use `early_stopping_rounds` during training to auto-pick optimal iteration\n",
    "- **Tip**: Start with basic settings and use `GridSearchCV` or `Optuna` for tuning\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089ef7a",
   "metadata": {},
   "source": [
    "<a id=\"neural-net\"></a>\n",
    "#### üß† Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5273e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is a Neural Network?\n",
    "\n",
    "A Neural Network is a layered structure of interconnected \"neurons\" inspired by the human brain.  \n",
    "Each neuron applies a weighted transformation followed by a non-linear activation, allowing the model to learn **complex, non-linear patterns** in the data.\n",
    "\n",
    "Even a basic feedforward neural network (also called Multi-Layer Perceptron or MLP) can approximate intricate decision boundaries ‚Äî making it powerful but harder to interpret.\n",
    "\n",
    "> Think of it as a flexible function builder that learns patterns layer by layer.\n",
    "\n",
    "##### ‚úÖ Pros vs ‚ùå Cons\n",
    "\n",
    "| Pros                                  | Cons                                   |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Can model complex, non-linear relationships | Requires lots of data and tuning     |\n",
    "| Works well on both tabular and image/text data | Not interpretable out of the box |\n",
    "| Scales with data and compute          | Can overfit if not regularized        |\n",
    "| Highly customizable architectures     | Slower to train, harder to debug      |\n",
    "\n",
    "##### üß† When to Use\n",
    "\n",
    "Use Neural Networks when:\n",
    "- You have **enough data** and want to model **complex interactions**\n",
    "- You're comfortable with longer training and tuning\n",
    "- You care more about **predictive power** than explainability\n",
    "- You're building pipelines that could benefit from **deep learning extensions** later\n",
    "\n",
    "##### ‚ö†Ô∏è Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Prone to overfitting ‚Äî always use dropout, regularization, or early stopping\n",
    "- **Hack**: Use a simple architecture (1‚Äì2 hidden layers) for structured/tabular data\n",
    "- **Tip**: Standardize inputs and tune learning rate; training can otherwise stall or explode\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7550b",
   "metadata": {},
   "source": [
    "<a id=\"model-exploration\"></a>\n",
    "# üìä Model Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d35c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "model_registry = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # needed for ROC AUC\n",
    "    # \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Neural Network\": MLPClassifier(max_iter=1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41656b",
   "metadata": {},
   "source": [
    "<a id=\"model-comparison\"></a>\n",
    "#### üìà Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score, confusion_matrix, log_loss\n",
    ")\n",
    "\n",
    "for name, model in model_registry.items():\n",
    "    print(f\"\\nüîß Training: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        y_scores = y_pred\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Metrics\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_scores)\n",
    "    specificity = tn / (tn + fp)\n",
    "    logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "    model_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"specificity\": specificity,\n",
    "        \"log_loss\": logloss\n",
    "    }\n",
    "\n",
    "    # Evaluations\n",
    "    plot_confusion(y_test, y_pred, model_name=name)\n",
    "    plot_roc_auc(model, X_test, y_test, model_name=name)\n",
    "    print_prf_metrics(y_test, y_pred, model_name=name)\n",
    "    print(\"‚Äî\" * 80)  # horizontal line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66adeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615179b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick based on 'accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity', 'log_loss'\n",
    "criteria = \"accuracy\"\n",
    "\n",
    "# Identify the best model based on selected criteria\n",
    "best_model_name = max(model_results, key=lambda x: model_results[x][criteria])\n",
    "best_model = model_results[best_model_name][\"model\"]\n",
    "best_score = model_results[best_model_name][criteria]\n",
    "\n",
    "print(f\"\\nüèÜ Best model so far: {best_model_name} ({criteria.upper()} = {best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ad2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual results dictionary\n",
    "df_results = pd.DataFrame(model_results).T\n",
    "\n",
    "# Metrics to visualize\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\n",
    "\n",
    "# Create subplot layout (3 rows x 2 columns)\n",
    "fig = sp.make_subplots(rows=3, cols=2, subplot_titles=[m.upper() for m in metrics])\n",
    "\n",
    "# Add bar chart to each subplot\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = divmod(i, 2)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_results.index,\n",
    "            y=df_results[metric],\n",
    "            name=metric,\n",
    "            text=pd.to_numeric(df_results[metric], errors=\"coerce\").round(3),  # value labels\n",
    "            textposition=\"auto\"\n",
    "        ),\n",
    "        row=row+1, col=col+1\n",
    "    )\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=1000,\n",
    "    title_text=\"Model Comparison by Metric\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdddf6c",
   "metadata": {},
   "source": [
    "<a id=\"feature-importance\"></a>\n",
    "#### üìä Feature Importance\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "Feature importance tells us **which variables the model relied on most** to make predictions.  \n",
    "It‚Äôs like asking, ‚ÄúWhat factors influenced the decision the most?‚Äù\n",
    "\n",
    "In tree-based models like Random Forest or XGBoost, it‚Äôs calculated based on how often and how effectively a feature was used to split the data.\n",
    "\n",
    "This is useful for:\n",
    "- Understanding the model‚Äôs decision logic\n",
    "- Identifying key business drivers\n",
    "- Eliminating irrelevant features\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=10):\n",
    "    \"\"\"\n",
    "    Plots top N feature importances from a tree-based model.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support feature_importances_\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(importance_df[\"Feature\"][::-1], importance_df[\"Importance\"][::-1])\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(best_model, X_train.columns, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2fc97",
   "metadata": {},
   "source": [
    "<a id=\"shap-values\"></a>\n",
    "#### üß¨ SHAP Values\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values explain **how much each feature contributed** to a specific prediction ‚Äî positively or negatively.\n",
    "\n",
    "It‚Äôs like breaking down a credit score:  \n",
    "> ‚ÄúAge added +12 points, income removed -5 points‚Ä¶‚Äù\n",
    "\n",
    "SHAP is model-agnostic and gives **local explanations** (for individual predictions) and **global insights** (feature impact across all predictions).\n",
    "\n",
    "Useful for:\n",
    "- Auditing high-stakes predictions\n",
    "- Building trust with stakeholders\n",
    "- Diagnosing model behavior case-by-case\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def plot_shap_summary_tree(model, X):\n",
    "    \"\"\"\n",
    "    Use TreeExplainer for tree-based models like RandomForest, XGBoost.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # For binary classification, use shap_values[1]\n",
    "    shap.summary_plot(shap_values[1], X)\n",
    "\n",
    "    print(\"\\nüìå Interpretation:\")\n",
    "    print(\"- Each bar shows how much that feature influences the model‚Äôs decision.\")\n",
    "    print(\"- Features at the top are the most impactful across all predictions.\")\n",
    "    print(\"- Blue/red indicate direction: does the feature push prediction up or down?\")\n",
    "    print(\"- Helps us understand *why* the model is confident ‚Äî not just *what* it predicts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_summary_tree(best_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ee97d",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72dd1f",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "# üõ†Ô∏è Fine-Tune the Winner\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "- Fine-tuning helps unlock the model‚Äôs full potential by finding better hyperparameter values.\n",
    "- It improves accuracy, recall, and other metrics without changing the model type.\n",
    "- We typically tune the best-performing model from the baseline round (Random Forest in our case).\n",
    "- Two common methods: Grid Search (exhaustive) and Randomized Search (faster, approximate).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec9b38",
   "metadata": {},
   "source": [
    "<a id=\"grid-search\"></a>\n",
    "#### üîé Grid Search\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is Grid Search?\n",
    "\n",
    "Grid Search tests **all possible combinations** of hyperparameters across a fixed grid.  \n",
    "It‚Äôs exhaustive, simple, and works best when the number of hyperparameters is small.\n",
    "\n",
    "- **Pros**: Comprehensive, easy to understand  \n",
    "- **Cons**: Very slow when search space is large\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# üîß Complete and default-aware param grid\n",
    "param_grids = {\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [100, 200],              # default: 100\n",
    "        \"max_depth\": [None, 5, 10],              # default: None\n",
    "        \"min_samples_split\": [2, 5],             # default: 2\n",
    "        \"min_samples_leaf\": [1, 2],              # default: 1\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]         # default: \"sqrt\"\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"max_depth\": [None, 5, 10],              # default: None\n",
    "        \"min_samples_split\": [2, 5],             # default: 2\n",
    "        \"min_samples_leaf\": [1, 2],              # default: 1\n",
    "        \"criterion\": [\"gini\", \"entropy\"]         # default: \"gini\"\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        # Note: Naive Bayes (GaussianNB) has limited tunable parameters ‚Äî only var_smoothing is exposed\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]      # default: 1e-9\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],                 # default: 1\n",
    "        \"penalty\": [\"l2\"],                       # default: \"l2\"\n",
    "        \"solver\": [\"lbfgs\"],                     # default: \"lbfgs\"\n",
    "        \"max_iter\": [100, 500]                   # default: 100\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"C\": [0.1, 1, 10],                       # default: 1\n",
    "        \"kernel\": [\"linear\", \"rbf\"],             # default: \"rbf\"\n",
    "        \"gamma\": [\"scale\", \"auto\"],              # default: \"scale\"\n",
    "        \"probability\": [True]                    # default: False (forced True for AUC)\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"n_neighbors\": [3, 5, 7],                # default: 5\n",
    "        \"weights\": [\"uniform\", \"distance\"],      # default: \"uniform\"\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]  # default: \"minkowski\"\n",
    "    },\n",
    "    \"MLPClassifier\": {\n",
    "        \"hidden_layer_sizes\": [(50,), (100,)],  # default: (100,)\n",
    "        \"activation\": [\"relu\", \"tanh\"],          # default: \"relu\"\n",
    "        \"alpha\": [0.0001, 0.001],                # default: 0.0001\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],  # default: \"constant\"\n",
    "        \"max_iter\": [200, 500]                   # default: 200\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, log_loss\n",
    ")\n",
    "\n",
    "# ‚öôÔ∏è Resolve model name and corresponding grid\n",
    "model_name = best_model.__class__.__name__\n",
    "param_grid = param_grids.get(model_name)\n",
    "\n",
    "if param_grid is None:\n",
    "    raise ValueError(f\"No param grid defined for model: {model_name}\")\n",
    "\n",
    "print(f\"\\nüîß Running Grid Search for: {model_name}\")\n",
    "\n",
    "# üß™ Run Grid Search\n",
    "model_instance = best_model.__class__()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_instance,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"‚úÖ Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# üìà Evaluate tuned model\n",
    "y_pred = best_tuned_model.predict(X_test)\n",
    "\n",
    "if hasattr(best_tuned_model, \"predict_proba\"):\n",
    "    y_scores = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(best_tuned_model, \"decision_function\"):\n",
    "    y_scores = best_tuned_model.decision_function(X_test)\n",
    "else:\n",
    "    y_scores = y_pred\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "specificity = tn / (tn + fp)\n",
    "logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "# Add to model_results with a new key\n",
    "model_results[f\"{model_name} (Tuned)\"] = {\n",
    "    \"model\": best_tuned_model,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"auc\": auc,\n",
    "    \"specificity\": specificity,\n",
    "    \"log_loss\": logloss\n",
    "}\n",
    "\n",
    "# Visual eval\n",
    "plot_confusion(y_test, y_pred, model_name=f\"{model_name} (Tuned)\")\n",
    "plot_roc_auc(best_tuned_model, X_test, y_test, model_name=f\"{model_name} (Tuned)\")\n",
    "print_prf_metrics(y_test, y_pred, model_name=f\"{model_name} (Tuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7778ad",
   "metadata": {},
   "source": [
    "<a id=\"random-search\"></a>\n",
    "#### üé≤ Randomized Search\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîç What is Randomized Search?\n",
    "\n",
    "Randomized Search selects a **random subset of combinations** to test, rather than all of them.  \n",
    "It‚Äôs faster and often just as effective ‚Äî especially when only a few hyperparameters really matter.\n",
    "\n",
    "- **Pros**: Much faster than grid search, good for large spaces  \n",
    "- **Cons**: May miss optimal combo if unlucky\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# üîÅ Use same param grid as defined earlier\n",
    "model_name = best_model.__class__.__name__\n",
    "param_dist = param_grids.get(model_name)\n",
    "\n",
    "if param_dist is None:\n",
    "    raise ValueError(f\"No param distribution defined for model: {model_name}\")\n",
    "\n",
    "print(f\"\\nüé≤ Running Randomized Search for: {model_name}\")\n",
    "\n",
    "# Create a new instance of the model\n",
    "model_instance = best_model.__class__()\n",
    "\n",
    "# üîç Run randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_instance,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,            # Number of random combos to try\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_random_model = random_search.best_estimator_\n",
    "\n",
    "print(\"‚úÖ Best Parameters Found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# üîé Evaluate tuned model\n",
    "y_pred = best_random_model.predict(X_test)\n",
    "\n",
    "if hasattr(best_random_model, \"predict_proba\"):\n",
    "    y_scores = best_random_model.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(best_random_model, \"decision_function\"):\n",
    "    y_scores = best_random_model.decision_function(X_test)\n",
    "else:\n",
    "    y_scores = y_pred\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "specificity = tn / (tn + fp)\n",
    "logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "# Store results\n",
    "model_results[f\"{model_name} (RandomSearch)\"] = {\n",
    "    \"model\": best_random_model,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"auc\": auc,\n",
    "    \"specificity\": specificity,\n",
    "    \"log_loss\": logloss\n",
    "}\n",
    "\n",
    "# Visual eval\n",
    "plot_confusion(y_test, y_pred, model_name=f\"{model_name} (RandomSearch)\")\n",
    "plot_roc_auc(best_random_model, X_test, y_test, model_name=f\"{model_name} (RandomSearch)\")\n",
    "print_prf_metrics(y_test, y_pred, model_name=f\"{model_name} (RandomSearch)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba105",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed90e9",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "# üîÄ Ensemble Methods\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üîÄ When Should You Use Ensemble Methods?\n",
    "\n",
    "Ensembles are useful when:\n",
    "- **Single models plateau** and can‚Äôt capture all patterns\n",
    "- You want to **boost performance** by combining strengths of multiple models\n",
    "- You observe **inconsistent results** across base models (e.g., one is good at recall, another at precision)\n",
    "- You need more **robust and stable** predictions across different datasets\n",
    "\n",
    "Use ensembles **after benchmarking individual models** ‚Äî they add complexity but often yield better generalization.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2ed11",
   "metadata": {},
   "source": [
    "<a id=\"voting-ensemble\"></a>\n",
    "#### üó≥Ô∏è Voting Classifier\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üó≥Ô∏è What is a Voting Classifier?\n",
    "\n",
    "A Voting Classifier combines predictions from multiple different models and makes a final decision based on **majority vote** (for classification) or **average prediction** (for regression).\n",
    "\n",
    "There are two main types:\n",
    "- **Hard Voting**: Chooses the class predicted by the most models.\n",
    "- **Soft Voting**: Averages predicted probabilities and chooses the most likely class.\n",
    "\n",
    "It‚Äôs like consulting multiple doctors and going with the consensus.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define voting type: 'hard' or 'soft'\n",
    "voting_type = 'hard'  # change to 'hard' if you want majority voting\n",
    "\n",
    "# Define the ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('dt', DecisionTreeClassifier()),\n",
    "        ('nb', GaussianNB())\n",
    "    ],\n",
    "    voting=voting_type\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(f\"üîß Training: Voting Classifier ({voting_type})\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_voting, model_name=f\"Voting Classifier ({voting_type})\")\n",
    "\n",
    "# Only plot ROC if model supports probability estimates\n",
    "if voting_type == 'soft':\n",
    "    plot_roc_auc(voting_clf, X_test, y_test, model_name=f\"Voting Classifier ({voting_type})\")\n",
    "\n",
    "print_prf_metrics(y_test, y_pred_voting, model_name=f\"Voting Classifier ({voting_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c54d41",
   "metadata": {},
   "source": [
    "<a id=\"stacking-ensemble\"></a>\n",
    "#### üß¨ Stacking Classifier\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üß¨ What is Stacking?\n",
    "\n",
    "Stacking involves training multiple models (called base models), and then using a **meta-model** to learn how to best combine their outputs.\n",
    "\n",
    "Example:\n",
    "- Base models: logistic regression, decision tree, SVM\n",
    "- Meta-model: another model that learns which base model to trust more for each kind of input\n",
    "\n",
    "It‚Äôs like having specialists give their opinions, and then a generalist makes the final call based on their inputs.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define base models\n",
    "base_estimators = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Define meta-model (final estimator)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Build stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=False,  # set to True if you want raw features included in meta-model input\n",
    "    cv=5                # internal cross-validation\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"üîß Training: Stacking Classifier\")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_stack, model_name=\"Stacking Classifier\")\n",
    "plot_roc_auc(stacking_clf, X_test, y_test, model_name=\"Stacking Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_stack, model_name=\"Stacking Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dc29f",
   "metadata": {},
   "source": [
    "<a id=\"bagging\"></a>\n",
    "#### ü™µ Bagging\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### ü™µ What is Bagging?\n",
    "\n",
    "**Bagging** (Bootstrap Aggregating) builds multiple versions of the same model (e.g., decision trees), each trained on a different random sample of the data.\n",
    "\n",
    "Then it combines their outputs (usually by voting or averaging) to reduce overfitting and variance.\n",
    "\n",
    "**Random Forest** is a popular example of bagging.\n",
    "\n",
    "It‚Äôs like asking the same expert multiple times under different conditions and averaging their answers.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f616b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier            # fast, default\n",
    "from sklearn.linear_model import LogisticRegression        # works well with linear patterns\n",
    "from sklearn.neighbors import KNeighborsClassifier         # unstable, benefits a lot from bagging\n",
    "from sklearn.svm import SVC                                # slow with bagging, use carefully\n",
    "from sklearn.naive_bayes import GaussianNB                 # rare with bagging (already stable)\n",
    "from sklearn.ensemble import RandomForestClassifier        # not recommended ‚Äî it's already bagged\n",
    "\n",
    "# Example usage:\n",
    "# base_estimator = LogisticRegression(max_iter=1000)\n",
    "# base_estimator = KNeighborsClassifier()\n",
    "# base_estimator = SVC(probability=True)\n",
    "# base_estimator = GaussianNB()\n",
    "\n",
    "# Define bagging classifier with decision trees\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,              # number of trees\n",
    "    max_samples=0.8,              # bootstrap sample size\n",
    "    max_features=1.0,             # use all features\n",
    "    random_state=42,\n",
    "    n_jobs=-1                     # parallel processing\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"üîß Training: Bagging Classifier\")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bag = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_bag, model_name=\"Bagging Classifier\")\n",
    "plot_roc_auc(bagging_clf, X_test, y_test, model_name=\"Bagging Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_bag, model_name=\"Bagging Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe09515",
   "metadata": {},
   "source": [
    "<a id=\"boosting\"></a>\n",
    "#### üöÄ Boosting\n",
    "\n",
    "<details>\n",
    "<summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "##### üöÄ What is Boosting?\n",
    "\n",
    "Boosting trains models **sequentially** ‚Äî each new model focuses on correcting the mistakes of the previous one.\n",
    "\n",
    "It gives more weight to errors and slowly builds a strong overall model by combining many weak ones.\n",
    "\n",
    "Popular examples: **XGBoost**, **AdaBoost**, **Gradient Boosting**\n",
    "\n",
    "Think of it as building knowledge step by step, learning from past failures to get better over time.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "\n",
    "# Define the boosting classifier\n",
    "boosting_clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,        # number of boosting rounds\n",
    "    learning_rate=0.1,       # step size shrinkage\n",
    "    max_depth=3,             # depth of each weak learner\n",
    "    subsample=1.0,           # can be <1.0 for stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"üîß Training: Boosting Classifier\")\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_boost = boosting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_boost, model_name=\"Boosting Classifier\")\n",
    "plot_roc_auc(boosting_clf, X_test, y_test, model_name=\"Boosting Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_boost, model_name=\"Boosting Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7ad90",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172574e5",
   "metadata": {},
   "source": [
    "<a id=\"export-deploy\"></a>\n",
    "# üì¶ Export & Deployment (Optional)\n",
    "\n",
    "<details><summary><strong>üìñ Click to Expand</strong></summary>\n",
    "\n",
    "- Save the final trained model to disk (e.g., `.pkl`, `.joblib`)\n",
    "- Export final evaluation metrics (e.g., to `.json` or `.csv`)\n",
    "- Package preprocessing steps if applicable (e.g., scalers, encoders)\n",
    "- Useful for handing off, sharing, or production integration\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed13494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create export folder if it doesn't exist\n",
    "os.makedirs(\"export\", exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, \"export/best_model.joblib\")\n",
    "\n",
    "# Save the evaluation metrics\n",
    "with open(\"export/metrics.json\", \"w\") as f:\n",
    "    json.dump(model_results[best_model_name], f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Model and metrics exported to /export/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5296004",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
