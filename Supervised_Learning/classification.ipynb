{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7946c46b",
   "metadata": {},
   "source": [
    "![Python](https://img.shields.io/badge/python-3.9-blue)\n",
    "![Status: Pending Migration](https://img.shields.io/badge/status-pending%20migration-orange)\n",
    "\n",
    "<a id=\"table-of-contents\"></a>\n",
    "# 📖 Classification\n",
    "\n",
    "[🧭 Objective](#-objective)\n",
    "- [📌 What is Classification?](#-what-is-classification)\n",
    "- [📦 Use Cases](#-use-cases)\n",
    "\n",
    "[📂 Data Setup](#-data-setup)\n",
    "- [📥 Load Dataset](#-load-dataset)\n",
    "- [📊 Data Characteristics Dictionary](#-data-characteristics-dictionary)\n",
    "- [🧹 Preprocessing](#-preprocessing)\n",
    "\n",
    "[🧪 Baseline Classifier Model](#-baseline-classifier-model)\n",
    "\n",
    "[📊 Model Evaluation](#-model-evaluation)\n",
    "- [📉 Confusion Matrix](#-confusion-matrix)\n",
    "- [📈 ROC Curve / AUC](#-roc-curve--auc)\n",
    "- [📏 Precision / Recall / F1](#-precision--recall--f1)\n",
    "\n",
    "[🔍 Models](#-models)\n",
    "- [📊 Logistic Regression](#-logistic-regression)\n",
    "- [🧮 Naive Bayes](#-naive-bayes)\n",
    "- [🌳 Decision Tree](#-decision-tree)\n",
    "- [🌲 Random Forest](#-random-forest)\n",
    "- [🎯 KNN (K-Nearest Neighbors)](#-knn-k-nearest-neighbors)\n",
    "- [📈 SVM (Support Vector Machines)](#-svm-support-vector-machines)\n",
    "- [🚀 XGBoost](#-xgboost)\n",
    "- [🧠 Neural Network](#-neural-network)\n",
    "\n",
    "[📊 Model Exploration](#-model-exploration)\n",
    "- [📈 Model Comparison](#-model-comparison)\n",
    "- [📊 Feature Importance](#-feature-importance)\n",
    "- [🧬 SHAP Values](#-shap-values)\n",
    "\n",
    "[🛠️ Fine-Tune the Winner](#-fine-tune-the-winner)\n",
    "- [🔎 Grid Search](#-grid-search)\n",
    "- [🎲 Randomized Search](#-randomized-search)\n",
    "\n",
    "[🔀 Ensemble Methods](#-ensemble-methods)\n",
    "- [🗳️ Voting Classifier](#-voting-classifier)\n",
    "- [🧬 Stacking Classifier](#-stacking-classifier)\n",
    "- [🪵 Bagging](#-bagging)\n",
    "- [🚀 Boosting](#-boosting)\n",
    "- [📦 Export & Deployment (Optional)](#-export--deployment-optional)\n",
    "\n",
    "<hr style=\"border: none; height: 1px; background-color: #ddd;\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6e3d2",
   "metadata": {},
   "source": [
    "<a id=\"objective\"></a>\n",
    "# 🧭 Objective\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafdd70",
   "metadata": {},
   "source": [
    "<a id=\"what-is-classification\"></a>\n",
    "#### 📌 What is Classification?\n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>\n",
    "Classification is a type of supervised machine learning where the goal is to predict a categorical label for an observation. Given a set of features (input data), the model tries to assign the observation to one of several predefined classes. Common examples of classification problems include:\n",
    "- **Spam detection**: Classifying emails as spam or not.\n",
    "- **Customer churn prediction**: Classifying customers as likely to leave (churn) or stay based on their activity.\n",
    "- **Image recognition**: Classifying images into categories, like identifying animals, vehicles, etc.\n",
    "\n",
    "In classification, the output is discrete (e.g., 'spam' vs 'not spam', 'churn' vs 'no churn'). This contrasts with regression, where the output is continuous (e.g., predicting a house price).\n",
    "\n",
    "##### Key Points\n",
    "- Supervised learning approach.\n",
    "- Used for predicting categories.\n",
    "- Output is discrete (binary or multiclass).\n",
    "- Examples: email classification, disease diagnosis, fraud detection.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8616d21",
   "metadata": {},
   "source": [
    "<a id=\"classification-use-cases\"></a>\n",
    "#### 📦 Use Cases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4415e",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133f37c7",
   "metadata": {},
   "source": [
    "<a id=\"data-setup\"></a>\n",
    "# 📂 Data Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a09beb6",
   "metadata": {},
   "source": [
    "<a id=\"load-dataset\"></a>\n",
    "#### 📥 Load Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c540ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine Learning and Model Evaluation\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Statistical and Other Utilities\n",
    "from scipy.stats import zscore\n",
    "from termcolor import colored\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faf2c74",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Click to Expand</strong></summary>\n",
    "In this section, we will begin by preparing the dataset. For simplicity, we'll use a simulated classification dataset generated using the `make_classification` function from `sklearn`. This allows us to create a synthetic dataset that is suitable for practicing classification tasks.\n",
    "\n",
    "We will simulate a dataset with the following properties:\n",
    "- 1000 samples (observations)\n",
    "- 10 features (predictors)\n",
    "- 2 informative features (ones that help in prediction)\n",
    "- 2 classes (binary classification problem)\n",
    "\n",
    "Let's generate and take a look at the data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca7b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Simulate base classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=2,\n",
    "    n_redundant=2,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    weights=[0.7, 0.3],  # simulate class imbalance\n",
    "    flip_y=0.01,         # 1% label noise\n",
    "    class_sep=0.8,       # less separation = harder task\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=[f\"Feature_{i}\" for i in range(1, 11)])\n",
    "target_col = \"Target\" \n",
    "df[target_col] = y\n",
    "\n",
    "# Inject missing values randomly (e.g., 1% of cells)\n",
    "# mask = np.random.rand(*df.shape) < 0.01\n",
    "# df[mask] = np.nan\n",
    "\n",
    "# Display preview\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28407bd",
   "metadata": {},
   "source": [
    "<a id=\"data-characteristics-dictionary\"></a>\n",
    "\n",
    "#### 📊 Data Characteristics Dictionary\n",
    "\n",
    "<details><summary><strong>📖 Click to Expand Explanation</strong></summary>\n",
    "\n",
    "This section initializes the **data characteristics dictionary**, which will store various metadata about the dataset, including details about the target variable, features, data size, and linear separability.\n",
    "\n",
    "The dictionary contains the following key sections:\n",
    "\n",
    "1. **🎯 Target Variable**:\n",
    "   - **Type**: Specifies whether the target variable is **binary** or **multiclass**.\n",
    "   - **Imbalance**: Indicates whether the target variable has **class imbalance**.\n",
    "   - **Class Imbalance Severity**: Specifies the severity of the imbalance (e.g., **high**, **low**).\n",
    "\n",
    "2. **🔧 Features**:\n",
    "   - **Type**: Describes the type of features in the dataset (e.g., **categorical**, **continuous**, or **mixed**).\n",
    "   - **Correlation**: Indicates the correlation between features (e.g., **low**, **medium**, **high**).\n",
    "   - **Outliers**: Flag to indicate whether **outliers** are detected in the features.\n",
    "   - **Missing Data**: Tracks the percentage of **missing data** or flags missing values.\n",
    "\n",
    "3. **📈 Data Size**:\n",
    "   - **Size**: Contains the **number of samples** (rows) and **number of features** (columns).\n",
    "\n",
    "4. **🔍 Linear Separability**:\n",
    "   - **Linear Separability**: States whether the classes are **linearly separable** (True or False).\n",
    "\n",
    "This dictionary will be updated dynamically as we analyze the dataset in subsequent steps. It serves as a **summary of key dataset properties** to help guide further analysis and modeling decisions.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73753c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the data characteristics dictionary\n",
    "data_characteristics = {\n",
    "    \"target_variable\": {\n",
    "        \"type\": None,  # \"binary\", \"multiclass\"\n",
    "        \"imbalance\": None,  # True if imbalanced, False otherwise\n",
    "        \"class_imbalance_severity\": None  # e.g., \"high\", \"low\"\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"type\": None,  # \"categorical\", \"continuous\", \"mixed\"\n",
    "        \"correlation\": None,  # \"low\", \"medium\", \"high\"\n",
    "        \"outliers\": None,  # True if outliers detected, False otherwise\n",
    "        \"missing_data\": None  # Percentage of missing data or boolean\n",
    "    },\n",
    "    \"data_size\": None,  # Size of dataset (samples, features)\n",
    "    \"linear_separability\": None  # True if classes are linearly separable\n",
    "}\n",
    "\n",
    "model_results = {}  # model performance dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffb3b8",
   "metadata": {},
   "source": [
    "<a id=\"preprocessing\"></a>\n",
    "#### 🧹 Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953ec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=target_col)\n",
    "y = df[target_col]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(\"✅ Data split complete:\")\n",
    "print(f\"Train size: {len(X_train)}, Test size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77810fb9",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be293f5",
   "metadata": {},
   "source": [
    "<a id=\"baseline-model\"></a>\n",
    "# 🧪 Baseline Classifier Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d9e15",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Click to Expand </strong></summary>\n",
    "\n",
    "In this section, we define the **baseline model** for the classification task. The baseline model is typically a **dummy model** that can be used to compare against more sophisticated models. Here, we use the **DummyClassifier**, which predicts the majority class, to set a baseline performance.\n",
    "\n",
    "The baseline model will help us assess if more advanced models (e.g., Random Forest, SVM) are making meaningful improvements over a simple strategy.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Fit a dummy classifier as a baseline\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")  # or try \"stratified\", \"uniform\"\n",
    "dummy_clf.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fe3d2",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "# 📊 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c780d645",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "- **Accuracy**: Overall correctness. Misleading when classes are imbalanced.\n",
    "- **Precision**: Of predicted positives, how many are truly positive? Important when false positives are costly.\n",
    "- **Recall**: Of actual positives, how many did we catch? Crucial when missing positives is expensive.\n",
    "- **F1 Score**: Harmonic mean of precision and recall. Useful when you care about balance.\n",
    "- **ROC AUC**: Probability a random positive ranks above a random negative. Good for probability-based classifiers.\n",
    "\n",
    "We'll log all metrics per model to enable direct comparisons during tuning or model selection.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d488821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Technical output\n",
    "print(\"📉 Technical Output (Classification Report)\\n\")\n",
    "print(classification_report(y_test, y_pred_dummy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c2cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine positive class label\n",
    "positive_class = y_train.unique()[1] if len(y_train.unique()) == 2 else 1\n",
    "\n",
    "# Compute metrics\n",
    "acc  = accuracy_score(y_test, y_pred_dummy)\n",
    "prec = precision_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "rec  = recall_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "f1   = f1_score(y_test, y_pred_dummy, pos_label=positive_class, average='binary', zero_division=0)\n",
    "\n",
    "# Print with padded % values to align arrows\n",
    "print(\"\\n📊 Business-Friendly Summary:\")\n",
    "print(f\"- Accuracy  : {acc :>7.2%} → Overall correctness. How many total predictions were right.\")\n",
    "print(f\"- Precision : {prec:>7.2%} → Of the cases we predicted as '{positive_class}', how many were actually '{positive_class}'.\")\n",
    "print(f\"- Recall    : {rec :>7.2%} → Of all the actual '{positive_class}' cases, how many did we correctly identify.\")\n",
    "print(f\"- F1 Score  : {f1  :>7.2%} → A balance between Precision and Recall. Higher means more reliable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72b6cba",
   "metadata": {},
   "source": [
    "<a id=\"confusion-matrix\"></a>\n",
    "#### 📉 Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1360e6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "The confusion matrix is a 2x2 table that helps us visualize the performance of a classification model.  \n",
    "Each cell represents the count of:\n",
    "\n",
    "- **True Positives (TP)**: Correctly predicted positive cases  \n",
    "- **False Positives (FP)**: Incorrectly predicted as positive  \n",
    "- **True Negatives (TN)**: Correctly predicted negative cases  \n",
    "- **False Negatives (FN)**: Missed positive cases (predicted as negative)\n",
    "\n",
    "To make interpretation easier, we also show **percentages** alongside the raw counts.\n",
    "\n",
    "We use this baseline matrix to see how well future models improve over this simple benchmark.\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5dcd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix with both count and percentage annotations.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm)\n",
    "    cm_perc = cm / cm_sum * 100\n",
    "\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            annot[i, j] = f\"{c}\\n({p:.1f}%)\"\n",
    "\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=annot, fmt=\"\", cmap=\"Blues\", cbar=True,\n",
    "                xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.title(f\"Confusion Matrix ({model_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(y_test, y_pred_dummy, model_name=\"Baseline Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9fd3c7",
   "metadata": {},
   "source": [
    "<a id=\"roc-auc\"></a>\n",
    "#### 📈 ROC Curve / AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ca9c4",
   "metadata": {},
   "source": [
    "<details><summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "**ROC Curve** (Receiver Operating Characteristic) plots the True Positive Rate (TPR) vs False Positive Rate (FPR) across different threshold values.\n",
    "\n",
    "- A model that randomly guesses would fall along the diagonal (AUC = 0.5)\n",
    "- A perfect model hugs the top-left corner (AUC = 1.0)\n",
    "\n",
    "**AUC (Area Under the Curve)** quantifies overall separability between the two classes:\n",
    "- **Technical Insight**: Higher AUC means better discrimination between positive and negative cases.\n",
    "- **Business Relevance**: Especially useful when false positives and false negatives have different costs — like fraud detection, churn prediction, etc.\n",
    "\n",
    "This plot lets stakeholders quickly gauge how good the model is — regardless of classification threshold.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_auc(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Plot ROC curve, print AUC score, and give business-facing interpretation.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support probability estimates or decision function.\")\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "    auc_score = roc_auc_score(y_test, y_scores)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.2f}\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random Guess\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(f\"ROC Curve ({model_name})\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Output\n",
    "    print(f\"🔹 ROC AUC Score for {model_name}: {auc_score:.4f}\")\n",
    "    if auc_score <= 0.55:\n",
    "        print(\"📌 Interpretation: Model performs at or near random. It cannot meaningfully separate classes.\")\n",
    "    elif auc_score < 0.7:\n",
    "        print(\"📌 Interpretation: Some separability, but not reliable yet. Needs improvement.\")\n",
    "    else:\n",
    "        print(\"📌 Interpretation: Model is doing a good job distinguishing between classes.\")\n",
    "\n",
    "plot_roc_auc(dummy_clf, X_test, y_test, model_name=\"Baseline Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40deee67",
   "metadata": {},
   "source": [
    "<a id=\"prf-metrics\"></a>\n",
    "#### 📏 Precision / Recall / F1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4741aba9",
   "metadata": {},
   "source": [
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "**Precision, Recall, and F1 Score** are classification metrics that help us understand model performance beyond just accuracy:\n",
    "\n",
    "- **Precision**: Of all predicted positives, how many were actually correct? (Low precision = many false alarms)\n",
    "- **Recall**: Of all actual positives, how many did we catch? (Low recall = missed positives)\n",
    "- **F1 Score**: Harmonic mean of precision and recall — useful when classes are imbalanced.\n",
    "\n",
    "**Business Perspective**:\n",
    "- If false positives are costly (e.g., spam filters, fraud flags), precision matters more.\n",
    "- If missing positives is risky (e.g., cancer detection), recall is critical.\n",
    "- F1 balances both and gives a single, interpretable metric.\n",
    "\n",
    "These metrics are vital when accuracy is misleading — especially in skewed datasets.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaae3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "def print_prf_metrics(y_true, y_pred, model_name=\"Model\", average=\"binary\"):\n",
    "    \"\"\"\n",
    "    Print precision, recall, and F1 score with both technical and business-friendly outputs.\n",
    "    \"\"\"\n",
    "    precision = precision_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, average=average, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=average, zero_division=0)\n",
    "\n",
    "    # Technical metrics\n",
    "    print(f\"🔹 Precision: {precision:.4f}\")\n",
    "    print(f\"🔹 Recall:    {recall:.4f}\")\n",
    "    print(f\"🔹 F1 Score:  {f1:.4f}\")\n",
    "\n",
    "    # Business interpretation\n",
    "    print(\"\\n📌 Interpretation:\")\n",
    "    if precision < 0.6:\n",
    "        print(\"- Model may generate too many false positives — not ideal where false alarms are costly.\")\n",
    "    else:\n",
    "        print(\"- Precision looks acceptable; false positives are relatively under control.\")\n",
    "\n",
    "    if recall < 0.6:\n",
    "        print(\"- Model is missing many actual positives — risky if false negatives have high cost.\")\n",
    "    else:\n",
    "        print(\"- Recall is strong; model is catching most of the true cases.\")\n",
    "\n",
    "    print(f\"- F1 Score indicates overall balance between precision and recall: {f1:.2f}\")\n",
    "\n",
    "print_prf_metrics(y_test, y_pred_dummy, model_name=\"Baseline Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd53b18",
   "metadata": {},
   "source": [
    "\n",
    "[Back to the top](#table-of-contents)\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb56812",
   "metadata": {},
   "source": [
    "<a id=\"models\"></a>\n",
    "# 🔍 Models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b01510",
   "metadata": {},
   "source": [
    "<a id=\"logistic-regression\"></a>\n",
    "#### 📊 Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96dcdb4",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is Logistic Regression?\n",
    "\n",
    "Despite the name, **Logistic Regression** is used for classification — not regression.  \n",
    "It predicts the **probability** that an observation belongs to a certain class (e.g., 0 or 1).  \n",
    "Under the hood, it fits a weighted formula to the input features, applies a sigmoid function, and outputs a value between 0 and 1.\n",
    "\n",
    "> Example:  \n",
    "> A model might say there's a **78% chance** this customer will churn.  \n",
    "> If that crosses a certain threshold (say, 50%), we classify it as “Yes.”\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                              | Cons                                  |\n",
    "|-----------------------------------|---------------------------------------|\n",
    "| Fast and efficient                | Assumes linear relationship (log-odds) |\n",
    "| Easy to interpret (feature weights) | Doesn’t handle complex patterns well  |\n",
    "| Works well with small datasets    | Sensitive to multicollinearity        |\n",
    "| Outputs probabilities             | May underperform on nonlinear data    |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use Logistic Regression when:\n",
    "- You want a **quick baseline** with interpretable output\n",
    "- You care about **probabilities**, not just labels\n",
    "- Your data is fairly **linearly separable**\n",
    "- The number of features is small to medium\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: If features are highly correlated (multicollinearity), the model may become unstable. Use regularization (e.g., L2 penalty).\n",
    "- **Hack**: For imbalanced datasets, adjust the threshold or use `class_weight='balanced'` to avoid bias toward the majority class.\n",
    "- **Tip**: Standardize features before training, especially if using regularization.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680be07",
   "metadata": {},
   "source": [
    "<a id=\"naive-bayes\"></a>\n",
    "#### 🧮 Naive Bayes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c52f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is Naive Bayes?\n",
    "\n",
    "Naive Bayes is a family of **probabilistic classifiers** based on Bayes’ Theorem.  \n",
    "It assumes that all features are **independent** of each other — which is rarely true in practice, but the model still performs surprisingly well.\n",
    "\n",
    "It calculates the probability of each class given the input features and picks the class with the highest likelihood.\n",
    "\n",
    "> Example:  \n",
    "> “Given these symptoms, what’s the most probable disease?” — Naive Bayes is widely used in text classification, spam detection, and medical diagnosis.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                               | Cons                                  |\n",
    "|------------------------------------|---------------------------------------|\n",
    "| Very fast and scalable             | Assumes feature independence (naive)  |\n",
    "| Handles high-dimensional data well | May underperform with correlated inputs |\n",
    "| Simple and interpretable           | Struggles with numeric feature scaling |\n",
    "| Works well with text data          | Outputs are often overconfident       |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use Naive Bayes when:\n",
    "- You’re working with **text** (e.g., spam filters, sentiment)\n",
    "- You want a **fast baseline**\n",
    "- You’re dealing with **high-dimensional**, sparse features (like TF-IDF)\n",
    "- You have clean categorical or binary features\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Doesn’t handle continuous features naturally — convert them to bins or use GaussianNB.\n",
    "- **Hack**: Apply **Laplace smoothing** to handle zero probabilities in unseen combinations.\n",
    "- **Tip**: Don’t expect high accuracy on raw numeric data — it shines in text-like scenarios.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a831589",
   "metadata": {},
   "source": [
    "<a id=\"decision-tree\"></a>\n",
    "#### 🌳 Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279d386",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is a Decision Tree?\n",
    "\n",
    "A Decision Tree splits data into branches based on feature values, creating a flowchart-like structure.  \n",
    "Each split is chosen to maximize class separation (typically using Gini impurity or entropy).  \n",
    "The result is a set of human-readable rules — like:  \n",
    "“If age < 30 and income > 50K → likely to churn.”\n",
    "\n",
    "It’s intuitive and easy to explain, even to non-technical stakeholders.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                             | Cons                                 |\n",
    "|----------------------------------|--------------------------------------|\n",
    "| Easy to visualize and interpret  | Prone to overfitting on noisy data   |\n",
    "| No need for feature scaling      | Can create unstable splits           |\n",
    "| Captures non-linear relationships | Doesn’t generalize well on small data |\n",
    "| Works for both numeric and categorical | Can be biased toward dominant features |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use Decision Trees when:\n",
    "- You need a model that’s **explainable** (e.g., in regulated domains)\n",
    "- Your data has **mixed types** (numeric + categorical)\n",
    "- You want to **prototype quickly** and understand feature importance\n",
    "- You’re okay with less predictive power in favor of interpretability\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Deep trees can memorize the training data — always prune or set `max_depth`.\n",
    "- **Hack**: Use as a weak learner inside ensembles (like Random Forest or XGBoost) to improve performance.\n",
    "- **Tip**: Use feature importance from trees to guide feature selection for other models.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad987ee3",
   "metadata": {},
   "source": [
    "<a id=\"random-forest\"></a>\n",
    "#### 🌲 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ca9ca",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is a Random Forest?\n",
    "\n",
    "Random Forest is an **ensemble method** that builds many decision trees and combines their outputs.  \n",
    "Each tree sees a random subset of the data and features, making the forest **diverse and robust**.\n",
    "\n",
    "It works by aggregating the predictions of multiple trees (majority vote for classification), reducing the overfitting risk of a single decision tree.\n",
    "\n",
    "> Think of it as a crowd of weak models working together to make better predictions.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                                  | Cons                                   |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Strong performance out of the box     | Less interpretable than a single tree  |\n",
    "| Handles non-linearities and interactions | Slower for real-time predictions     |\n",
    "| Resistant to overfitting              | May require tuning to perform well     |\n",
    "| Works well with large feature spaces  | Not ideal when interpretability is key |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use Random Forest when:\n",
    "- You need a **reliable general-purpose model** with minimal tuning\n",
    "- You want to **improve stability** over a single decision tree\n",
    "- Your data is **tabular and structured**\n",
    "- You care more about **performance** than full interpretability\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: May become large and slow — tune `n_estimators` and `max_depth` if needed\n",
    "- **Hack**: Use `feature_importances_` to find influential variables\n",
    "- **Tip**: Avoid one-hot encoding with high-cardinality features — use label encoding instead\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a4b23",
   "metadata": {},
   "source": [
    "<a id=\"knn\"></a>\n",
    "#### 🎯 KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e97a2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is K-Nearest Neighbors?\n",
    "\n",
    "KNN is a **non-parametric**, instance-based learning method.  \n",
    "It doesn’t learn a model during training — instead, it stores the data.  \n",
    "At prediction time, it looks at the **K most similar observations** (neighbors) and assigns the class based on majority vote.\n",
    "\n",
    "Similarity is usually measured using **Euclidean distance** (or other distance metrics for different data types).\n",
    "\n",
    "> Example:  \n",
    "> “To predict a label for this point, look at its 5 closest data points and choose the most common class.”\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                                 | Cons                                      |\n",
    "|--------------------------------------|-------------------------------------------|\n",
    "| Simple and intuitive                 | Slow at prediction time (no training step) |\n",
    "| No training required                 | Struggles with high-dimensional data       |\n",
    "| Captures local patterns              | Requires feature scaling                  |\n",
    "| Flexible distance metrics            | Memory-intensive with large datasets      |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use KNN when:\n",
    "- You have **low-dimensional**, clean data\n",
    "- You want to **prototype quickly** with minimal assumptions\n",
    "- You care about **local behavior** rather than global rules\n",
    "- Interpretability is less important than flexibility\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Distance metrics break down in high-dimensional space (curse of dimensionality)\n",
    "- **Hack**: Use `StandardScaler` or `MinMaxScaler` to normalize features before fitting\n",
    "- **Tip**: Tune `k` using cross-validation; odd numbers help avoid ties in binary classification\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550a922",
   "metadata": {},
   "source": [
    "<a id=\"svm\"></a>\n",
    "#### 📈 SVM (Support Vector Machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fbd637",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is SVM?\n",
    "\n",
    "Support Vector Machines (SVM) are **margin-based classifiers** that try to find the best boundary (hyperplane) that separates classes.  \n",
    "SVM focuses on **support vectors** — the critical data points closest to the boundary — to maximize the margin between classes.\n",
    "\n",
    "It can handle **non-linear patterns** using kernel tricks (e.g., RBF kernel), making it flexible for complex data.\n",
    "\n",
    "> Think of it as drawing the widest possible gap between two classes while avoiding overlap.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                                 | Cons                                   |\n",
    "|--------------------------------------|----------------------------------------|\n",
    "| Works well in high-dimensional spaces | Slow on large datasets                 |\n",
    "| Effective for non-linear boundaries   | Requires careful parameter tuning      |\n",
    "| Robust to overfitting (with regularization) | Not intuitive to interpret         |\n",
    "| Supports different kernels            | Doesn’t scale well with noisy data     |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use SVM when:\n",
    "- Your data is **high-dimensional**, but you want a **non-linear model**\n",
    "- You need a **strong classifier** and have time to tune hyperparameters\n",
    "- Dataset is **moderate in size** and reasonably clean\n",
    "- You care about maximizing **margin of separation**\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Doesn't output probabilities by default — use `probability=True` in `SVC` if needed\n",
    "- **Hack**: Use **RBF kernel** as a good starting point for non-linear problems\n",
    "- **Tip**: Always standardize features — SVM is sensitive to feature scale\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb6069",
   "metadata": {},
   "source": [
    "<a id=\"xgboost\"></a>\n",
    "#### 🚀 XGBoost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f2f79e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is XGBoost?\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful **boosted tree ensemble** method.  \n",
    "Unlike Random Forest (which builds trees in parallel), XGBoost builds trees **sequentially** — each new tree tries to fix the errors of the previous one.\n",
    "\n",
    "It uses **gradient descent** to minimize loss, with regularization to prevent overfitting.  \n",
    "XGBoost is known for its **speed, accuracy, and efficiency**, making it a go-to model in many Kaggle competitions and production systems.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                                 | Cons                                   |\n",
    "|--------------------------------------|----------------------------------------|\n",
    "| High predictive accuracy             | Harder to interpret                    |\n",
    "| Built-in regularization (less overfitting) | More complex than basic tree models |\n",
    "| Fast and scalable                    | Requires tuning for best performance   |\n",
    "| Handles missing data automatically   | May overfit small/noisy datasets       |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use XGBoost when:\n",
    "- You need **top-tier performance** on structured/tabular data\n",
    "- You’re working with **noisy or complex relationships**\n",
    "- You’re okay with a black-box model in exchange for results\n",
    "- You want built-in tools for **feature importance**, early stopping, etc.\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Easy to overfit if `n_estimators` is too high — always monitor with validation\n",
    "- **Hack**: Use `early_stopping_rounds` during training to auto-pick optimal iteration\n",
    "- **Tip**: Start with basic settings and use `GridSearchCV` or `Optuna` for tuning\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9089ef7a",
   "metadata": {},
   "source": [
    "<a id=\"neural-net\"></a>\n",
    "#### 🧠 Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd5273e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is a Neural Network?\n",
    "\n",
    "A Neural Network is a layered structure of interconnected \"neurons\" inspired by the human brain.  \n",
    "Each neuron applies a weighted transformation followed by a non-linear activation, allowing the model to learn **complex, non-linear patterns** in the data.\n",
    "\n",
    "Even a basic feedforward neural network (also called Multi-Layer Perceptron or MLP) can approximate intricate decision boundaries — making it powerful but harder to interpret.\n",
    "\n",
    "> Think of it as a flexible function builder that learns patterns layer by layer.\n",
    "\n",
    "##### ✅ Pros vs ❌ Cons\n",
    "\n",
    "| Pros                                  | Cons                                   |\n",
    "|---------------------------------------|----------------------------------------|\n",
    "| Can model complex, non-linear relationships | Requires lots of data and tuning     |\n",
    "| Works well on both tabular and image/text data | Not interpretable out of the box |\n",
    "| Scales with data and compute          | Can overfit if not regularized        |\n",
    "| Highly customizable architectures     | Slower to train, harder to debug      |\n",
    "\n",
    "##### 🧠 When to Use\n",
    "\n",
    "Use Neural Networks when:\n",
    "- You have **enough data** and want to model **complex interactions**\n",
    "- You're comfortable with longer training and tuning\n",
    "- You care more about **predictive power** than explainability\n",
    "- You're building pipelines that could benefit from **deep learning extensions** later\n",
    "\n",
    "##### ⚠️ Pitfalls & Hacks\n",
    "\n",
    "- **Pitfall**: Prone to overfitting — always use dropout, regularization, or early stopping\n",
    "- **Hack**: Use a simple architecture (1–2 hidden layers) for structured/tabular data\n",
    "- **Tip**: Standardize inputs and tune learning rate; training can otherwise stall or explode\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7550b",
   "metadata": {},
   "source": [
    "<a id=\"model-exploration\"></a>\n",
    "# 📊 Model Exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d35c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "model_registry = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"SVM\": SVC(probability=True),  # needed for ROC AUC\n",
    "    # \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Neural Network\": MLPClassifier(max_iter=1000)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41656b",
   "metadata": {},
   "source": [
    "<a id=\"model-comparison\"></a>\n",
    "#### 📈 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20950ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    accuracy_score, roc_auc_score, confusion_matrix, log_loss\n",
    ")\n",
    "\n",
    "for name, model in model_registry.items():\n",
    "    print(f\"\\n🔧 Training: {name}\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_scores = model.predict_proba(X_test)[:, 1]\n",
    "    elif hasattr(model, \"decision_function\"):\n",
    "        y_scores = model.decision_function(X_test)\n",
    "    else:\n",
    "        y_scores = y_pred\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Metrics\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_scores)\n",
    "    specificity = tn / (tn + fp)\n",
    "    logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "    model_results[name] = {\n",
    "        \"model\": model,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "        \"specificity\": specificity,\n",
    "        \"log_loss\": logloss\n",
    "    }\n",
    "\n",
    "    # Evaluations\n",
    "    plot_confusion(y_test, y_pred, model_name=name)\n",
    "    plot_roc_auc(model, X_test, y_test, model_name=name)\n",
    "    print_prf_metrics(y_test, y_pred, model_name=name)\n",
    "    print(\"—\" * 80)  # horizontal line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66adeda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615179b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick based on 'accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity', 'log_loss'\n",
    "criteria = \"accuracy\"\n",
    "\n",
    "# Identify the best model based on selected criteria\n",
    "best_model_name = max(model_results, key=lambda x: model_results[x][criteria])\n",
    "best_model = model_results[best_model_name][\"model\"]\n",
    "best_score = model_results[best_model_name][criteria]\n",
    "\n",
    "print(f\"\\n🏆 Best model so far: {best_model_name} ({criteria.upper()} = {best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ad2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "import pandas as pd\n",
    "\n",
    "# Replace with your actual results dictionary\n",
    "df_results = pd.DataFrame(model_results).T\n",
    "\n",
    "# Metrics to visualize\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'specificity']\n",
    "\n",
    "# Create subplot layout (3 rows x 2 columns)\n",
    "fig = sp.make_subplots(rows=3, cols=2, subplot_titles=[m.upper() for m in metrics])\n",
    "\n",
    "# Add bar chart to each subplot\n",
    "for i, metric in enumerate(metrics):\n",
    "    row, col = divmod(i, 2)\n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=df_results.index,\n",
    "            y=df_results[metric],\n",
    "            name=metric,\n",
    "            text=pd.to_numeric(df_results[metric], errors=\"coerce\").round(3),  # value labels\n",
    "            textposition=\"auto\"\n",
    "        ),\n",
    "        row=row+1, col=col+1\n",
    "    )\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    height=900,\n",
    "    width=1000,\n",
    "    title_text=\"Model Comparison by Metric\",\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdddf6c",
   "metadata": {},
   "source": [
    "<a id=\"feature-importance\"></a>\n",
    "#### 📊 Feature Importance\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "Feature importance tells us **which variables the model relied on most** to make predictions.  \n",
    "It’s like asking, “What factors influenced the decision the most?”\n",
    "\n",
    "In tree-based models like Random Forest or XGBoost, it’s calculated based on how often and how effectively a feature was used to split the data.\n",
    "\n",
    "This is useful for:\n",
    "- Understanding the model’s decision logic\n",
    "- Identifying key business drivers\n",
    "- Eliminating irrelevant features\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93ec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_feature_importance(model, feature_names, top_n=10):\n",
    "    \"\"\"\n",
    "    Plots top N feature importances from a tree-based model.\n",
    "    \"\"\"\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        importances = model.feature_importances_\n",
    "    else:\n",
    "        raise ValueError(\"Model does not support feature_importances_\")\n",
    "\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.barh(importance_df[\"Feature\"][::-1], importance_df[\"Importance\"][::-1])\n",
    "    plt.title(\"Top Feature Importances\")\n",
    "    plt.xlabel(\"Importance Score\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(best_model, X_train.columns, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2fc97",
   "metadata": {},
   "source": [
    "<a id=\"shap-values\"></a>\n",
    "#### 🧬 SHAP Values\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) values explain **how much each feature contributed** to a specific prediction — positively or negatively.\n",
    "\n",
    "It’s like breaking down a credit score:  \n",
    "> “Age added +12 points, income removed -5 points…”\n",
    "\n",
    "SHAP is model-agnostic and gives **local explanations** (for individual predictions) and **global insights** (feature impact across all predictions).\n",
    "\n",
    "Useful for:\n",
    "- Auditing high-stakes predictions\n",
    "- Building trust with stakeholders\n",
    "- Diagnosing model behavior case-by-case\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "def plot_shap_summary_tree(model, X):\n",
    "    \"\"\"\n",
    "    Use TreeExplainer for tree-based models like RandomForest, XGBoost.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # For binary classification, use shap_values[1]\n",
    "    shap.summary_plot(shap_values[1], X)\n",
    "\n",
    "    print(\"\\n📌 Interpretation:\")\n",
    "    print(\"- Each bar shows how much that feature influences the model’s decision.\")\n",
    "    print(\"- Features at the top are the most impactful across all predictions.\")\n",
    "    print(\"- Blue/red indicate direction: does the feature push prediction up or down?\")\n",
    "    print(\"- Helps us understand *why* the model is confident — not just *what* it predicts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3470ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_summary_tree(best_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11ee97d",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c72dd1f",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "# 🛠️ Fine-Tune the Winner\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "- Fine-tuning helps unlock the model’s full potential by finding better hyperparameter values.\n",
    "- It improves accuracy, recall, and other metrics without changing the model type.\n",
    "- We typically tune the best-performing model from the baseline round (Random Forest in our case).\n",
    "- Two common methods: Grid Search (exhaustive) and Randomized Search (faster, approximate).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec9b38",
   "metadata": {},
   "source": [
    "<a id=\"grid-search\"></a>\n",
    "#### 🔎 Grid Search\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is Grid Search?\n",
    "\n",
    "Grid Search tests **all possible combinations** of hyperparameters across a fixed grid.  \n",
    "It’s exhaustive, simple, and works best when the number of hyperparameters is small.\n",
    "\n",
    "- **Pros**: Comprehensive, easy to understand  \n",
    "- **Cons**: Very slow when search space is large\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb1470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 🔧 Complete and default-aware param grid\n",
    "param_grids = {\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [100, 200],              # default: 100\n",
    "        \"max_depth\": [None, 5, 10],              # default: None\n",
    "        \"min_samples_split\": [2, 5],             # default: 2\n",
    "        \"min_samples_leaf\": [1, 2],              # default: 1\n",
    "        \"max_features\": [\"sqrt\", \"log2\"]         # default: \"sqrt\"\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"max_depth\": [None, 5, 10],              # default: None\n",
    "        \"min_samples_split\": [2, 5],             # default: 2\n",
    "        \"min_samples_leaf\": [1, 2],              # default: 1\n",
    "        \"criterion\": [\"gini\", \"entropy\"]         # default: \"gini\"\n",
    "    },\n",
    "    \"GaussianNB\": {\n",
    "        # Note: Naive Bayes (GaussianNB) has limited tunable parameters — only var_smoothing is exposed\n",
    "        \"var_smoothing\": [1e-9, 1e-8, 1e-7]      # default: 1e-9\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10],                 # default: 1\n",
    "        \"penalty\": [\"l2\"],                       # default: \"l2\"\n",
    "        \"solver\": [\"lbfgs\"],                     # default: \"lbfgs\"\n",
    "        \"max_iter\": [100, 500]                   # default: 100\n",
    "    },\n",
    "    \"SVC\": {\n",
    "        \"C\": [0.1, 1, 10],                       # default: 1\n",
    "        \"kernel\": [\"linear\", \"rbf\"],             # default: \"rbf\"\n",
    "        \"gamma\": [\"scale\", \"auto\"],              # default: \"scale\"\n",
    "        \"probability\": [True]                    # default: False (forced True for AUC)\n",
    "    },\n",
    "    \"KNeighborsClassifier\": {\n",
    "        \"n_neighbors\": [3, 5, 7],                # default: 5\n",
    "        \"weights\": [\"uniform\", \"distance\"],      # default: \"uniform\"\n",
    "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]  # default: \"minkowski\"\n",
    "    },\n",
    "    \"MLPClassifier\": {\n",
    "        \"hidden_layer_sizes\": [(50,), (100,)],  # default: (100,)\n",
    "        \"activation\": [\"relu\", \"tanh\"],          # default: \"relu\"\n",
    "        \"alpha\": [0.0001, 0.001],                # default: 0.0001\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\"],  # default: \"constant\"\n",
    "        \"max_iter\": [200, 500]                   # default: 200\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26dae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, log_loss\n",
    ")\n",
    "\n",
    "# ⚙️ Resolve model name and corresponding grid\n",
    "model_name = best_model.__class__.__name__\n",
    "param_grid = param_grids.get(model_name)\n",
    "\n",
    "if param_grid is None:\n",
    "    raise ValueError(f\"No param grid defined for model: {model_name}\")\n",
    "\n",
    "print(f\"\\n🔧 Running Grid Search for: {model_name}\")\n",
    "\n",
    "# 🧪 Run Grid Search\n",
    "model_instance = best_model.__class__()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model_instance,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_tuned_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"✅ Best Parameters Found:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# 📈 Evaluate tuned model\n",
    "y_pred = best_tuned_model.predict(X_test)\n",
    "\n",
    "if hasattr(best_tuned_model, \"predict_proba\"):\n",
    "    y_scores = best_tuned_model.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(best_tuned_model, \"decision_function\"):\n",
    "    y_scores = best_tuned_model.decision_function(X_test)\n",
    "else:\n",
    "    y_scores = y_pred\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "specificity = tn / (tn + fp)\n",
    "logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "# Add to model_results with a new key\n",
    "model_results[f\"{model_name} (Tuned)\"] = {\n",
    "    \"model\": best_tuned_model,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"auc\": auc,\n",
    "    \"specificity\": specificity,\n",
    "    \"log_loss\": logloss\n",
    "}\n",
    "\n",
    "# Visual eval\n",
    "plot_confusion(y_test, y_pred, model_name=f\"{model_name} (Tuned)\")\n",
    "plot_roc_auc(best_tuned_model, X_test, y_test, model_name=f\"{model_name} (Tuned)\")\n",
    "print_prf_metrics(y_test, y_pred, model_name=f\"{model_name} (Tuned)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7778ad",
   "metadata": {},
   "source": [
    "<a id=\"random-search\"></a>\n",
    "#### 🎲 Randomized Search\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔍 What is Randomized Search?\n",
    "\n",
    "Randomized Search selects a **random subset of combinations** to test, rather than all of them.  \n",
    "It’s faster and often just as effective — especially when only a few hyperparameters really matter.\n",
    "\n",
    "- **Pros**: Much faster than grid search, good for large spaces  \n",
    "- **Cons**: May miss optimal combo if unlucky\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f07e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# 🔁 Use same param grid as defined earlier\n",
    "model_name = best_model.__class__.__name__\n",
    "param_dist = param_grids.get(model_name)\n",
    "\n",
    "if param_dist is None:\n",
    "    raise ValueError(f\"No param distribution defined for model: {model_name}\")\n",
    "\n",
    "print(f\"\\n🎲 Running Randomized Search for: {model_name}\")\n",
    "\n",
    "# Create a new instance of the model\n",
    "model_instance = best_model.__class__()\n",
    "\n",
    "# 🔍 Run randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model_instance,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=15,            # Number of random combos to try\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "best_random_model = random_search.best_estimator_\n",
    "\n",
    "print(\"✅ Best Parameters Found:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# 🔎 Evaluate tuned model\n",
    "y_pred = best_random_model.predict(X_test)\n",
    "\n",
    "if hasattr(best_random_model, \"predict_proba\"):\n",
    "    y_scores = best_random_model.predict_proba(X_test)[:, 1]\n",
    "elif hasattr(best_random_model, \"decision_function\"):\n",
    "    y_scores = best_random_model.decision_function(X_test)\n",
    "else:\n",
    "    y_scores = y_pred\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_scores)\n",
    "specificity = tn / (tn + fp)\n",
    "logloss = log_loss(y_test, y_scores)\n",
    "\n",
    "# Store results\n",
    "model_results[f\"{model_name} (RandomSearch)\"] = {\n",
    "    \"model\": best_random_model,\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"auc\": auc,\n",
    "    \"specificity\": specificity,\n",
    "    \"log_loss\": logloss\n",
    "}\n",
    "\n",
    "# Visual eval\n",
    "plot_confusion(y_test, y_pred, model_name=f\"{model_name} (RandomSearch)\")\n",
    "plot_roc_auc(best_random_model, X_test, y_test, model_name=f\"{model_name} (RandomSearch)\")\n",
    "print_prf_metrics(y_test, y_pred, model_name=f\"{model_name} (RandomSearch)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dba105",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ed90e9",
   "metadata": {},
   "source": [
    "<a id=\"ensemble\"></a>\n",
    "# 🔀 Ensemble Methods\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🔀 When Should You Use Ensemble Methods?\n",
    "\n",
    "Ensembles are useful when:\n",
    "- **Single models plateau** and can’t capture all patterns\n",
    "- You want to **boost performance** by combining strengths of multiple models\n",
    "- You observe **inconsistent results** across base models (e.g., one is good at recall, another at precision)\n",
    "- You need more **robust and stable** predictions across different datasets\n",
    "\n",
    "Use ensembles **after benchmarking individual models** — they add complexity but often yield better generalization.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed2ed11",
   "metadata": {},
   "source": [
    "<a id=\"voting-ensemble\"></a>\n",
    "#### 🗳️ Voting Classifier\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🗳️ What is a Voting Classifier?\n",
    "\n",
    "A Voting Classifier combines predictions from multiple different models and makes a final decision based on **majority vote** (for classification) or **average prediction** (for regression).\n",
    "\n",
    "There are two main types:\n",
    "- **Hard Voting**: Chooses the class predicted by the most models.\n",
    "- **Soft Voting**: Averages predicted probabilities and chooses the most likely class.\n",
    "\n",
    "It’s like consulting multiple doctors and going with the consensus.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6d42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define voting type: 'hard' or 'soft'\n",
    "voting_type = 'hard'  # change to 'hard' if you want majority voting\n",
    "\n",
    "# Define the ensemble\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('lr', LogisticRegression(max_iter=1000)),\n",
    "        ('dt', DecisionTreeClassifier()),\n",
    "        ('nb', GaussianNB())\n",
    "    ],\n",
    "    voting=voting_type\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(f\"🔧 Training: Voting Classifier ({voting_type})\")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_voting, model_name=f\"Voting Classifier ({voting_type})\")\n",
    "\n",
    "# Only plot ROC if model supports probability estimates\n",
    "if voting_type == 'soft':\n",
    "    plot_roc_auc(voting_clf, X_test, y_test, model_name=f\"Voting Classifier ({voting_type})\")\n",
    "\n",
    "print_prf_metrics(y_test, y_pred_voting, model_name=f\"Voting Classifier ({voting_type})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c54d41",
   "metadata": {},
   "source": [
    "<a id=\"stacking-ensemble\"></a>\n",
    "#### 🧬 Stacking Classifier\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🧬 What is Stacking?\n",
    "\n",
    "Stacking involves training multiple models (called base models), and then using a **meta-model** to learn how to best combine their outputs.\n",
    "\n",
    "Example:\n",
    "- Base models: logistic regression, decision tree, SVM\n",
    "- Meta-model: another model that learns which base model to trust more for each kind of input\n",
    "\n",
    "It’s like having specialists give their opinions, and then a generalist makes the final call based on their inputs.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80f3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define base models\n",
    "base_estimators = [\n",
    "    ('lr', LogisticRegression(max_iter=1000)),\n",
    "    ('dt', DecisionTreeClassifier()),\n",
    "    ('nb', GaussianNB())\n",
    "]\n",
    "\n",
    "# Define meta-model (final estimator)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Build stacking classifier\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=False,  # set to True if you want raw features included in meta-model input\n",
    "    cv=5                # internal cross-validation\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"🔧 Training: Stacking Classifier\")\n",
    "stacking_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels\n",
    "y_pred_stack = stacking_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_stack, model_name=\"Stacking Classifier\")\n",
    "plot_roc_auc(stacking_clf, X_test, y_test, model_name=\"Stacking Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_stack, model_name=\"Stacking Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dc29f",
   "metadata": {},
   "source": [
    "<a id=\"bagging\"></a>\n",
    "#### 🪵 Bagging\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🪵 What is Bagging?\n",
    "\n",
    "**Bagging** (Bootstrap Aggregating) builds multiple versions of the same model (e.g., decision trees), each trained on a different random sample of the data.\n",
    "\n",
    "Then it combines their outputs (usually by voting or averaging) to reduce overfitting and variance.\n",
    "\n",
    "**Random Forest** is a popular example of bagging.\n",
    "\n",
    "It’s like asking the same expert multiple times under different conditions and averaging their answers.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f616b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier            # fast, default\n",
    "from sklearn.linear_model import LogisticRegression        # works well with linear patterns\n",
    "from sklearn.neighbors import KNeighborsClassifier         # unstable, benefits a lot from bagging\n",
    "from sklearn.svm import SVC                                # slow with bagging, use carefully\n",
    "from sklearn.naive_bayes import GaussianNB                 # rare with bagging (already stable)\n",
    "from sklearn.ensemble import RandomForestClassifier        # not recommended — it's already bagged\n",
    "\n",
    "# Example usage:\n",
    "# base_estimator = LogisticRegression(max_iter=1000)\n",
    "# base_estimator = KNeighborsClassifier()\n",
    "# base_estimator = SVC(probability=True)\n",
    "# base_estimator = GaussianNB()\n",
    "\n",
    "# Define bagging classifier with decision trees\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=50,              # number of trees\n",
    "    max_samples=0.8,              # bootstrap sample size\n",
    "    max_features=1.0,             # use all features\n",
    "    random_state=42,\n",
    "    n_jobs=-1                     # parallel processing\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"🔧 Training: Bagging Classifier\")\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_bag = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_bag, model_name=\"Bagging Classifier\")\n",
    "plot_roc_auc(bagging_clf, X_test, y_test, model_name=\"Bagging Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_bag, model_name=\"Bagging Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe09515",
   "metadata": {},
   "source": [
    "<a id=\"boosting\"></a>\n",
    "#### 🚀 Boosting\n",
    "\n",
    "<details>\n",
    "<summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "##### 🚀 What is Boosting?\n",
    "\n",
    "Boosting trains models **sequentially** — each new model focuses on correcting the mistakes of the previous one.\n",
    "\n",
    "It gives more weight to errors and slowly builds a strong overall model by combining many weak ones.\n",
    "\n",
    "Popular examples: **XGBoost**, **AdaBoost**, **Gradient Boosting**\n",
    "\n",
    "Think of it as building knowledge step by step, learning from past failures to get better over time.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de5cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "\n",
    "# Define the boosting classifier\n",
    "boosting_clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,        # number of boosting rounds\n",
    "    learning_rate=0.1,       # step size shrinkage\n",
    "    max_depth=3,             # depth of each weak learner\n",
    "    subsample=1.0,           # can be <1.0 for stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the ensemble\n",
    "print(\"🔧 Training: Boosting Classifier\")\n",
    "boosting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_boost = boosting_clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "plot_confusion(y_test, y_pred_boost, model_name=\"Boosting Classifier\")\n",
    "plot_roc_auc(boosting_clf, X_test, y_test, model_name=\"Boosting Classifier\")\n",
    "print_prf_metrics(y_test, y_pred_boost, model_name=\"Boosting Classifier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7ad90",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172574e5",
   "metadata": {},
   "source": [
    "<a id=\"export-deploy\"></a>\n",
    "# 📦 Export & Deployment (Optional)\n",
    "\n",
    "<details><summary><strong>📖 Click to Expand</strong></summary>\n",
    "\n",
    "- Save the final trained model to disk (e.g., `.pkl`, `.joblib`)\n",
    "- Export final evaluation metrics (e.g., to `.json` or `.csv`)\n",
    "- Package preprocessing steps if applicable (e.g., scalers, encoders)\n",
    "- Useful for handing off, sharing, or production integration\n",
    "\n",
    "</details>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed13494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create export folder if it doesn't exist\n",
    "os.makedirs(\"export\", exist_ok=True)\n",
    "\n",
    "# Save the best model\n",
    "joblib.dump(best_model, \"export/best_model.joblib\")\n",
    "\n",
    "# Save the evaluation metrics\n",
    "with open(\"export/metrics.json\", \"w\") as f:\n",
    "    json.dump(model_results[best_model_name], f, indent=2)\n",
    "\n",
    "print(\"✅ Model and metrics exported to /export/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5296004",
   "metadata": {},
   "source": [
    "[Back to the top](#table-of-contents)\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
