{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24bfbf9b",
   "metadata": {},
   "source": [
    "# Index\n",
    "- Setup\n",
    "- Classification\n",
    "    - [Naive Bayes](#Naive-Bayes)\n",
    "    - [Decision Tree](#Decision-Tree)\n",
    "- [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f98a6d",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c0335a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548757ce",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "448d9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros: simple and fast, efficient for high dim data, low training time, binary or MC,\n",
    "# high bias low variance - less prone to underfitting (feature independence), interpretable,  \n",
    "# numeric and categorical, \n",
    "# ensemble friendly but unlikely, online or batch\n",
    " \n",
    "# cons\n",
    "# sensitive to noise in data, bad for imbalanced data, \n",
    "\n",
    "# Pitfalls and troubleshoot\n",
    "# laplace smoothing\n",
    "\n",
    "# assumptions\n",
    "# assumes indep features, \n",
    "\n",
    "# preprocessing needed - missing values, categs, \n",
    "\n",
    "# variants\n",
    "# clf = MultinomialNB(class_prior=None, class_weight=class_weights)\n",
    "# clf = MultinomialNB(alpha=1) - laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0473a37f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[1;32m      4\u001b[0m clf \u001b[38;5;241m=\u001b[39m GaussianNB()\n\u001b[0;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]])\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Binary labels (0 or 1)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "clf = GaussianNB()\n",
    "X = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])\n",
    "y = np.array([0, 0, 1, 1, 1])  # Binary labels (0 or 1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee3cf38",
   "metadata": {},
   "source": [
    "[Back to the top](#Index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13285d40",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8aae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pros: \n",
    "# interpretable (white-box), less preprocessing (scaling, outlier, categ), non-linear relationships: decision boundary\n",
    "# offers feature importance, classification & regression\n",
    "# binary or mc, compute efficient, smooth bias variance tradeoff, somwehat robust to noise, \n",
    "\n",
    "# cons\n",
    "# overfitting when not pruned, high variance, bias toward dominant class, \n",
    "# less stable (sensitive dependent on training data, different tree), ~ greedy algorithm\n",
    "# batch, but online possible\n",
    "\n",
    "# Pitfalls and troubleshoot\n",
    "# overfitting - prune and limit depth or increase max sampels, \n",
    "# feature importance may be distorted for correlated fields - permutation importance or SHAP values\n",
    "\n",
    "# assumptions: Feature Independence (but ok), Binary Splits, Recursive Splittings,\n",
    "# Sequential Decision-Making\n",
    "\n",
    "# variants: non-binary splits, \n",
    "\n",
    "# Notes:\n",
    "# algo, feature importace, imbalanced, \n",
    "# weighted for minority class,\n",
    "# clf = DecisionTreeClassifier(class_weight={0: 1.0, 1: 10.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fda623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choice of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e776d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f4b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RF ####\n",
    "# Pros: high accuracy, outlier and noise robust, feature importance, handles missing data, \n",
    "# non-linear relationships, compute friendly given parallel, less overfitting, \n",
    "# non-parametric: no prior distribition assumed, \n",
    " \n",
    "# cons\n",
    "# high memory, not interpretable, needs large dataset preferably, \n",
    "# inflated feature importance - Bias Toward Features with Many Categories\n",
    "# sensitive towards imbalanced data\n",
    "\n",
    "# assumptions\n",
    "# Independence of Trees, randomness, ensemble principle, \n",
    "\n",
    "# preprocessing needed\n",
    "# multicollinearity\n",
    "\n",
    "# variants\n",
    "# clf = RandomForestClassifier(class_weight={0: 1.0, 1: 2.0, 2: 1.5})\n",
    "# ensemble pruning - discard poor trees\n",
    "\n",
    "# Implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e341c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### k-Nearest Neighbors ####\n",
    "# Pros: simple, no assumptions of underlying data - non-param, cls/reg, streaming-no training time, \n",
    "# Pros: Interpretable, robust to outlier, multiclass\n",
    "# binary or multiclass, \n",
    "\n",
    "# cons: sensitive to k, compute efficiency (approx nearest n, KD tree, ball tree, local hash) - multicore, dim redn,  GPU,  \n",
    "# sensitive to class imbalance, sensitive to outliers, missing imputation needed\n",
    "# sensitive to noise if small k, batch >> online\n",
    "\n",
    "# Pitfalls and troubleshoot\n",
    "# use with ensemble\n",
    "\n",
    "# assumptions: all features are equally important\n",
    "\n",
    "# variants: approx nearest neighbors?\n",
    "\n",
    "# Notes: scaling needed. Kvalue: CV/grid-search, domain knowledge, odd number\n",
    "# Small \"k\" (Low Bias, High Variance), underfitting, sensitive to noise, strong local patterns \n",
    "# anamoly detection: large distances\n",
    "# weights: distance, uniform, rank\n",
    "\n",
    "# imbalanced class: weights by class\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, weights='distance', class_weight={0: 1, 1: 10})\n",
    "\n",
    "# Implementation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3) \n",
    "knn_classifier.fit(X_train, y_train)\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_regressor.fit(X_train, y_train)\n",
    "y_pred = knn_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5ebfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Support Vector Machines ####\n",
    "# Pros\n",
    "# high dimensions, binary and MC, classification and regression, \n",
    "# immune to overfitting especially when regularised (tune C parameter)\n",
    "# global optimum, non linear relationships well handled with complex decision boundary maximising margin (RBF), \n",
    "# immune to outliers and noise,  \n",
    "# ensemble friendly but unlikely, online or batch\n",
    "\n",
    "# cons\n",
    "# black-box\n",
    "# compute costly, memory intensive, parameter tuning dependent, \n",
    "# not so good for MC problems, not interpretable, \n",
    "# class imbalance sensitive, \n",
    "\n",
    "# assumptions\n",
    "# linearly separable boundary (kernel trick), max margin (hyperplane and boundary points, )\n",
    "\n",
    "# preprocessing needed\n",
    "# scaling must\n",
    "\n",
    "# variants\n",
    "from sklearn.svm import SVC\n",
    "classifiers = {\n",
    "    \"Linear\": SVC(kernel='linear'),\n",
    "    \"Polynomial\": SVC(kernel='poly', degree=3),  # You can adjust the degree parameter\n",
    "    \"RBF\": SVC(kernel='rbf')\n",
    "}\n",
    "for kernel_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "# svm_classifier = SVC(kernel='linear', class_weight={0: 1.0, 1: 1.0, 2: 2.0})\n",
    "\n",
    "# notes:\n",
    "# cost parameter: small C wide margin underfit, large C overfit\n",
    "\n",
    "# Implementation\n",
    "svm_classifier = SVC(kernel='rbf', C=1, gamma='scale')\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7f72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Multinomial Logistic Regression ####\n",
    "# Pros: interpretable, compute efficient, converges quickly and online friendly, \n",
    "# L1 and L2 regularization possible, ensemble friendly,\n",
    "# good balance of bias and variance, \n",
    "\n",
    "# cons\n",
    "# linear relationships need transformation, not readily suitable for complex non-linear data\n",
    "# sensitive to outliers, requires feature engineering\n",
    "# parametric\n",
    "\n",
    "# assumptions\n",
    "# features are independent so are errors, \n",
    "# linearity of logit function, no multicollinearity, \n",
    "\n",
    "# preprocessing needed: missing, outlier, feature selection, categ encoding, \n",
    "# scaling optional but recommended for faster convergence?, \n",
    "\n",
    "# variants\n",
    "# model = LogisticRegression(class_weight={0: 1, 1: 5})\n",
    "\n",
    "# Implementation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standardize your features (optional but can be helpful)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ec7357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f2522c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### one vs rest (ovr) ####\n",
    "# good for few classes. But creates artificial imbalance for larger classes\n",
    "classifiers = []\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    y_binary = (y_train == i).astype(int)    \n",
    "    classifier = LogisticRegression()\n",
    "    classifier.fit(X_train, y_binary)\n",
    "    classifiers.append(classifier)\n",
    "\n",
    "class_predictions = []\n",
    "for classifier in classifiers:\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    class_predictions.append(y_pred)\n",
    "final_predictions = [max(range(num_classes), key=lambda x: class_predictions[x][i]) for i in range(len(X_test))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfef13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72653ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### one vs one: requires sufficient data ####\n",
    "classifiers = []\n",
    "for class_pair in class_pairs:\n",
    "    # Filter the training data for the current class pair\n",
    "    mask = (y_train == class_pair[0]) | (y_train == class_pair[1])\n",
    "    X_pair = X_train[mask]\n",
    "    y_pair = y_train[mask]\n",
    "    \n",
    "    # Train a binary classifier (e.g., Support Vector Machine)\n",
    "    classifier = SVC(kernel='linear')\n",
    "    classifier.fit(X_pair, y_pair)\n",
    "    \n",
    "    # Append the trained classifier to the list\n",
    "    classifiers.append(classifier)\n",
    "# Make predictions using all the binary classifiers\n",
    "predictions = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    predictions.append(y_pred)\n",
    "# Aggregate the binary classifier results to make the final prediction\n",
    "final_predictions = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    votes = [0] * len(set(y))\n",
    "    for j in range(len(class_pairs)):\n",
    "        if predictions[j][i] == class_pairs[j][0]:\n",
    "            votes[class_pairs[j][0]] += 1\n",
    "        else:\n",
    "            votes[class_pairs[j][1]] += 1\n",
    "    final_predictions.append(max(range(len(votes)), key=lambda x: votes[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d623b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3e627",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Bagging ####\n",
    "# sample w replacement, train model on them\n",
    "# parallel    \n",
    "    \n",
    "#### Boosting ####\n",
    "# models are trained sequentially, and each new model focuses on the previously misclassified samples \n",
    "\n",
    "#### Boosting - Gradient Boosting Classification ####\n",
    "# ensemble of decision trees sequentially, with each tree correcting the errors of the previous one. \n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbt_classifier = GradientBoostingClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "gbt_classifier.fit(X_train, y_train)\n",
    "y_pred = gbt_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0067bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Boosting - Gradient Boosting Trees ####\n",
    "# same as above\n",
    "# n_estimators - no of iterations to boost\n",
    "# learning_rate - 0.1 \n",
    "# max_depth - 3\n",
    "# random_state - None\n",
    "# loss - deviance or exponential\n",
    "# subsample - fraction of samples\n",
    "# min_samples_split - \n",
    "# min_samples_leaf - \n",
    "# max_features - sqrt, log2, none\n",
    "# max_leaf_nodes - \n",
    "# n_iter_no_change and tol\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd92ff3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c604de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Boosting - light GBM ####\n",
    "import lightgbm as lgb\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "# Define your LightGBM parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "clf = lgb.train(params, train_data, num_boost_round=100)\n",
    "y_pred = clf.predict(X_test, num_iteration=clf.best_iteration)\n",
    "\n",
    "boosting_type # gbdt, dart, goss\n",
    "objective: binary, multiclass, regression\n",
    "metric: binary_logloss, multi_logloss, rmse\n",
    "learning_rate\n",
    "n_estimators\n",
    "max_depth, num_leaves, min_child_samples, \n",
    "subsample\n",
    "reg_alpha\n",
    "reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76818b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Efficient Handling of Large Datasets:\n",
    "Faster Training Speed: \n",
    "Low memory usage\n",
    "GPU and multicore\n",
    "Early Stopping Capability: faster convergence\n",
    "\n",
    "Categorical, outliers and missing\n",
    "Improved Accuracy: leaf-first approach vs depth first\n",
    "Flexibility: custom objective function\n",
    "Support for Regularization: L1 and L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4dca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Boosting - Gradient Boosting Decision Trees ####\n",
    "\n",
    "#### Boosting - Gradient Boosting Regression Trees ####\n",
    "\n",
    "#### Boosting - Gradient Boosting Machine ####\n",
    "\n",
    "#### Boosting - Multiple Additive Regression Trees ####\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7caf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Boosting - XG Boost ####\n",
    "#Classification and Regression\n",
    "#efficiency (multicore)\n",
    "#accuracy\n",
    "#robustness (dtypes, sizes)\n",
    "#Missing values handled, k-fold and feature importance built in\n",
    "#L1 and L2 regularization to control overfitting\n",
    "\n",
    "# pip install xgboost\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',  # Regression task\n",
    "    'max_depth': 3,                  # Maximum depth of trees\n",
    "    'learning_rate': 0.1,            # Learning rate\n",
    "    'n_estimators': 100              # Number of boosting rounds (trees)\n",
    "}\n",
    "model = xgb.train(params, dtrain)\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "#### Boosting - ADA Boost ####\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "y_pred = adaboost_classifier.predict(X_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
